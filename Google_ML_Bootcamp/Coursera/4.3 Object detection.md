## 4.3 Object detection

### 1. Object localization
- 단순히 이미지를 받는 걸 넘어 객체의 위치를 바운딩 박스로 지정  
  ex) 자동차 위치 빨간 사각형으로 표시
- 바운딩 박스 -> (bx, by)- 중심 좌표 (bh, bw)- 높이와 너비
  - pc: 객체 존재 여부 (1: 존재, 0: 없음)
  - bx, by, bh, bw: 객체의 바운딩 박스 좌표, 크기
  - c1, c2, c3: 객체의 클래스 (보행자, 자동차, 오토바이) 한 번에 하나의 클래스만 선택
### 2. Landmark Detection
- 이미지에서 중요한 랜드마크 X,Y 좌표를 찾는 것
- 얼굴 인식, 포즈 인식
- ex) 얼굴 인식의 경우, 눈의 시작점 끝점, 입의 시작점 끝점 등등등 64개의 점을 찍는다고 하면  
  face?, 1x, 1y, ... , 64x, 64y -> 총 129개 label 생김
- 일괄된 랜드마크를 입력해줌으로써 랜드마크 탐지에 정확성을 높이고 감정인식, AR 효과 적용 등 다양하게 활용 가능


### 3. 객체감지 알고리즘
- Sliding Windows Detection Algorithm
  - 슬라이딩 윈도우 : 빨간 네모를 만들어 이미지를 통과시키는 것 - 점차 빨간 네모 크기를 키워가며 탐지  
    -> 큰 단점 : 계산 비용이 높음
  - 큰 stride 사용하면 성능 저하, 작은 stride 사용하면 비효율적으로 계산 비용 오름
  - 그래서 시작된 게 convolution 접근, 슬라이딩 윈도우 탐지 알고리즘을 컨볼루션 방식으로 효율적으로 구현

- sliding window 에 컨볼루션 구현하기
  - 슬라이딩 윈도우 방식 문제점-> 이미지를 작게 잘라 각각 결과 따로 계산 -> 계산 반복 수행 -> 비효율  
    해결하고자 -> 컨볼루션 이용 -> 잘라낸 부분 영역 공통 사용 계산을 공유  
    -> ex) 16x16 을 14x14 로 슬라이딩 윈도우 하면 4번 해야함 -> 컨볼루션 네트워크 이용해서 1번 연산 <-> 4개 결과  
      ![image](https://github.com/user-attachments/assets/fe4fd264-2ae7-4e4b-b4cd-daa2dfd96ee9)  
  - **4번 연산**하고 컨볼루션 해서 얻는 **4개의 결과값**이나  
    컨볼루션해서 **1번만 연산**하고 나오는 **4개의 결과값**이나 **같으**니까  
    컨볼루션을 먼저 해서 **연산줄이고 효율높이는** 개념

  - 컨볼루션 방식으로 슬라이딩 윈도우를 구현하면 훨씬 **효율적으로 객체 감지를 수행**  
    but, 바운딩 박스 위치의 정확도가 떨어지는 단점

### 4. YOLO algorithm (You Only Look Once)
- 빠르고 정확한 객체감지(Object Detection) 기술
- 정밀한 바운딩 박스를 출력
- 그리드 셀로 이미지 분할  
  ex) 8차원 출력 벡터 -> 3x3 격자판 -> 출력 **전체 볼륨 3x3x8**  
  ![image](https://github.com/user-attachments/assets/1d2b9768-19bb-4960-b684-b703376809de)  
  - P<sub>c</sub> : 객체 유무  
    B<sub>x</sub>, B<sub>y</sub> : 박스 중심 좌표 (0<= 중심좌표 <=1) 박스의 좌측 상단 = (0,0), 박스의 우측 하단 = (1,1)  
    B<sub>h</sub>, B<sub>w</sub> : 박스 높이, 너비 (때에 따라, 1보다 클수도 있음) 박스를 넘는 큰 객체일 가능성 있음  
    C<sub>1</sub>, C<sub>2</sub>, C<sub>3</sub> : class (보행자, 자동차, 오토바이)

- YOLO 알고리즘의 **장점**
  - 효율성: YOLO는 **한 번의 연산**으로 이미지 전체를 처리, 객체 감지 속도 **빠름**
  - 정확성: 바운딩 박스를 직접 출력하여 **다양한 비율**의 바운딩 박스 생성 가능
  - 실시간 감지: 매우 **빠른 속도**로 작동, **실시간 객체 감지**에 적합

- [ YOLO 논문 ] - Redmon et al., 2015, You Only Look Once: Unified real-time object detection 

### 5. IoU (intersection over union)
- 어떻게 객체 탐지 잘되는지 알 수 있지?
  - 합집합에 대한 교집합 (intersection over union) 이라는 함수 **IoU를 지표**로 삼아  
    관습적으로 **IoU > 0.5**를 기준으로,  
    종종 엄격히 0.6, 0.7로 보기도. 그래도 0.5보다 작게는 안봄.
- ex) 노란색 교집합 구간 size / 초록색 합집합 구간 size  
  ![image](https://github.com/user-attachments/assets/b2499f2f-96b1-416f-8d31-0778bb193954)


### 6. Non-max suppression
- 객체 감지 기술 대다수가 **여러 개의 감지가 중복**되게 일어날 수 있다는 **단점**이 있음  
  그걸 해결하기 위한 방법이 **non-max suppression = 최대값이 아닌 것을 억제**
- 이 box 안에 객체의 중심이 있어 라고 착각하는 box가 여러 개  
  ![image](https://github.com/user-attachments/assets/9b6337f4-c77d-4b5a-a049-ceb54c979dae)  
  박스 안에 객체 중심이 있을 확률(P<sub>c</sub>)이 0.6 가 넘는 경우가 0.7, 0.8 2가지 발생 -> max 값인 0.8을 채택
1. 확률값(P<sub>c</sub>)이 일정 임계값(예: 0.6) 이하인 바운딩 박스는 모두 제거
2. 남아 있는 바운딩 박스 중에서 가장 확률값이 높은 것을 선택 - 그 box = 최종 검출
3. 최종 검출과 높은 IoU를 가진(최종 검출과 많이 겹치는) 나머지 박스들은 억제하고, 제거
4. 최종 검출 이외에 남는 박스가 없을 때까지 반복

### 7. Anchor boxes
- 하나의 그리드 셀에 객체의 중심이 2개 들어가 있는 경우의 문제해결을 위해 앵커 박스  
  (class는 한개만 1 할 수 있으므로 - 2개 1하면 객체 크기나 이런 다른 변수를 올바르게 넣을 수 없으니까)  
  ![image](https://github.com/user-attachments/assets/12f204c3-112b-4341-af3c-6a3b229f7361)  

- 기존 3x3 그리드를 이용해 **3x3x8 출력 벡터**였으나, 앵커박스 2개를 이용해 **3x3x16 출력 벡터**로 변경하는 것 (**또는 3x3x2x8**)  
  (실제로 격자가 3x3보다 훨씬 크기 때문에 겹치는 일은 자주 발생하지 않고, 3개 4개 겹치는 일은 거의 없는 일이기는 하다)

### YOLO 알고리즘 실행 예제 (YOLO - Anchor - IoU - Non-max) 
  ![image](https://github.com/user-attachments/assets/f8ab6dff-f0be-47a7-9885-9f7b4f889133)

### R-CNN
슬라이딩 윈도우 -> 객체가 없을 확률이 높아도 연산을 해서 **비효율적**  
  -> R-CNN (Regions with CNNs) 지역 제안  
  -> segmentation algorithm 이용해서 바운딩 박스 출력  
    -> 단점 : 속도가 느림 -> 극복하기 위해 Fast R-CNN 제안 (전통 segmentation 대신 convolution 이용해서 지역 제안)
- YOLO 는 한번의 연산으로 모든 걸 동시 처리해서 R-CNN보다 효율적 = 장기적으로 더 유망  
  하지만 R-CNN도 중요한 아이디어이므로 이해할 필요 있음


### 8. Semantic Segmentation
- Object Recognition: 이미지를 입력받아 특정 객체가 있는지 판단하는 작업
  Object Detection: 객체를 인식할 뿐만 아니라, 해당 객체의 경계를 나타내는 바운딩 박스를 그리는 작업
  Semantic Segmentation: 객체를 인식하고 바운딩 박스를 그리는 것뿐만 아니라, 객체의 정확한 경계를 찾아 모든 픽셀 단위로 분류하는 작업

![image](https://github.com/user-attachments/assets/b50f6603-ca6b-419d-a485-96087eddeae8)
- 주로 자율주행에 쓰이고, 어디가 운전한지 안전한지 판단  
  의료 분야에서도 쓰이는에 사진이나 영상에서 기관이나 종양의 경계를 명확히 구분해, 진단과 수술을 도움
- 이 과정의 알고리즘 단위 : U-Net
- segmentation map  
  ![image](https://github.com/user-attachments/assets/2080b56b-8944-4949-9407-103f014236bb)
- 기존 convolution 신경망에서 y_hat 부분을 지우고 Transpose Convolution 이용해서 다시 size를 키워서 만듬  
  ![image](https://github.com/user-attachments/assets/07d5120d-a5ac-4188-9120-e8816ef82792)
