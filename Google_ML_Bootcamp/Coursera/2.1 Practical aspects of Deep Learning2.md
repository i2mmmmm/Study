### 3. 정규화

- 과적합 의심 = high variance  
  ->가장 먼저 해야할 일 = 정규화  
  -> 분산이 높다면 더 많은 training 데이터를 얻는 방법도 있지만 늘 얻을 수는 없으므로  
  정규화를 추가하는 것이 과적합을 방지하거나 분산을 줄이는 데에 도움이 된다.  
- ex) logistic regression  
  min J(w,b)  
  여기에서는 w에 대한 정규화만 진행  
  -> b는 그냥 단수인데 비해 w는 고차원이므로  
  -> L2 정규화  ℓ
  
  <p>J(w,b) = $\frac{1}{m} Σ^{i=1~m}(ŷ^{(i)},y^{(i)}) + \frac{λ}{2m} ||w||^{2} 　　 + <\frac{λ}{2m}b^2>-b 부분은 일단 생략$  </p>

- L1 정규화 : 절댓값 합계를 사용
- L2 정규화 : 제곱합을 사용

- L1 정규화의 경우, w 값이 희박해지고, 많은 w 벡터 값들이 0이 되므로, 아주 특정 소수의 모델에서 사용.  
  = L2 정규화가 더 많이 사용됨
- λ = regularization parameter
- Frobenius norm = 매트릭스 요소의 제곱 합
- L2 정규화를 가끔 weight decay(가중 감소) 라고 일컫는다.  
  w[l] (1 - (αλ/m)) 1보다 조금 작은 값으로 가중치 감소가 되는 이유


### 4. 정규화가 과적합을 방지한다고 하는 이유

- λ를 아주 크게 만들면 **w를 0에** 가깝도록 유도 가능 = **가중치를 0에** 가깝게 설정 가능 = 은닉층의 유닛들의 많은 영향을 **제로화**  
  -> **더 작은 신경망이 된다.**  = [ high variance -> high bias ] 형태로 변환  
그러니 그 중간쯤 **적당한 λ** 를 찾는다면 이상적으로 변환 가능

- ex) tanh activation function  
  g(z) = tanh(z)  
  z 값이 작다면 선형의 형태를 띄겠지만 z 값 아주 크거나 아주 작으면 덜 선형적인 형태  
  **λ** = 정규화 파라미터 가 **커질수록** 실제 파라미터인 w<sup>[l]</sup>이 **작아지게** 될 것  
z<sup>[l]</sup> = w<sup>[l]</sup>a<sup>[l-1]</sup>+b<sup>[l]</sup>  
  -> w 값이 작아지니 z 값도 작아지고  
  -> g(z)는 선형이 되고  
  -> 선형회귀처럼 보이게 됨  
  그럼 봤던 과적합그래프처럼 복잡한 고도의 그래프가 아닌 **선형**이 된다는 것이므로 = **과적합 방지**

- 정규화를 도입하는 경우  
  비용함수 J 를 정의하고 실제로 추가했는데 가중치가 너무 크면 벌점을 주는 이 제곱합 형태의 **L2 정규화 항을 추가**하자
- 경사하강법 디버그 단계 중 하나로 기울기가 점진적으로 감소하는 형태를 띄지 않으면 L2 정규화를 진행해본다.

다음 시간엔 드롭아웃 정규화
