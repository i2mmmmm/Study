#### 4. 수치적 Gradient 근사 (two-side)

- 역전파 (backpropagation) : 네트워크의 가중치를 업데이트하여 **예측 오류를 최소화**하는 방법
- 역전파를 구현할 때 구현이 제대로 됐는지 확인하는 방법 중 하나 : **Gradient checking**
- Gradient checking을 하기 위해 알아야 할 것!
  - 수치적 기울기 산출 (수치적 그래디언트 근사)

ex) f(θ) = θ^3　　(θ = 1, ε = 0.01 일 때) 

1. one-side  
f(1) = 1, f'(1) = 3, f(θ+ε)-f(θ)/ε ≒ **3.03**

2. two-side  
f(1) = 1, f'(1) = 3, f(θ+ε)-f(θ-ε)/2ε ≒ **3.0001**

**one-side** 로 하면 오차가 **0.03**, **two-side** 로 하면 오차가 **0.0001**  
two-side 근사가 보다 정확하므로, Gradient checking 에서  two-side(양쪽 차이로 기울기 근사 사용

#### 5. Gradient checking (Grad check)
- 시간 절약하도록 도와준 기법
- 어떻게 디버그 하는지 알아보기
- 역전파가 옳은지도 검증해보기

1. **모든 파라미터 벡터화**
   - W 행렬을 벡터로 변환하고, 모든 W와 B를 결합하여 θ 생성
2. **dθ<sub>approx</sub> 계산하기**
   - dθ<sub>approx</sub> [i] = J(θ1+θ2+...+θi + ε) - J(θ1+θ2+...+θi - ε) /2ε

3. **둘 사이의 거리 확인하기**
   - dθ<sub>approx</sub> ≈ dθ <- 이걸 확인하는 작업 = **Grad check**
   - <p> $\frac{||dθ_{approx} - dθ||_2}{||dθ_{approx}||_2 + ||dθ||_2} $</p>
   - 분자의 벡터가 너무 큰 값 or 작은 값을 가질까봐 분모가 비율로 맞춰줌

4. 이 때 **ε = 10<sup>-7</sup>** 정도면 정확하다고 판단

결론 : 신경망 모델 구축 -> 순전파 도입 -> 역전파 도입 -> 기울기 검사(Grad check) -> debug debug debug debug debug -> 작은 값이 나오면 끝

#### 6. Gradient Checking Tip
1. 훈련 단계에서 사용하지 말고 디버그용으로 사용하기
- 모든 값에 대해 계산하면 느린 작업 -> 디버깅 때만 근접한지 확인하고 끄고 훈련 계속

2. 알고리즘이 기울기검사에 실패한 경우, 요소들을 보기
- 특정 레이어의 db 또는 dw 에서 차이가 있는지 확인해서 버그 위치 추측하기

3. 정규화하는 경우 정규화 항 포함하기
- J(θ) 에 정규화 항 꼭 포함되어 있어야 기울기가 정확해짐


4. 기울기 검사는 dropout에서 작동하지 않음
- dropout 랜덤성으로 J 값 계산 어려움 -> 드롭아웃없이 기울기 검사를 하고 옳은 알고리즘이면 그 때 드롭아웃 켜기

5. w와 b가 0에 가까울 때 역전파 구현 정확
- w,b가 크다면 무작위 초기화 후 Grad Check 진행, 훈련 시키다가 다시 Grad Check
