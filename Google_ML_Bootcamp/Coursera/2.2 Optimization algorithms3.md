## 2.2 Optimization algorithms

### 3. momentum (gradient descent with momentum)

- Gradient descent 의 문제점이
  - 경사가 가파를수록 진동생겨서 느려지고
  - 학습률 높이면 발산할 수도 있고
  
  -> 그래서 momentum 을 도입
- momentum의 아이디어는 **기울기의 지수 가중 평균**을 구해서 그걸 이용해서 **가중을 업데이트** 하는 것
- 일반적인 경사 하강법에서는 dw, db를 사용하지만  
  모멘텀을 사용한 경사 하강법에서는 vdW, vdB를 사용
- 수직 방향의 진동을 줄여주고 수평 방향으로 빠르게 이동할 수 있게 돕는
- 밥그릇 모양의 함수에 공이 가운데로 굴러들어가는 듯
- 수식은
  - v<sub>dW</sub> = β v<sub>dW</sub> + (1-β) dW
  - v<sub>db</sub> = β v<sub>db</sub> + (1-β) db
    - 가중치 업데이트 (W = W - αv<sub>dW</sub>, b = b - αv<sub>db</sub>)
  - 하이퍼 파라미터는 α, β 두가지 인데 가장 흔한 β 값 = 0.9
    - = 최근 10일 간의 평균 기온 산출 = 지난 10개의 기울기 하강 평균치
    - 다른 β값 설정해봐도 되지만 0.9는 꽤 잘 작동하는 값
    - 바이어스 수정 : 10개 지나면 미분항 워밍업되어서 바이어스 추정치가 아니기 때문에  
      사람들이 바이어스 수정을 잘 신경쓰지 않는다.
- 문헌에서는 (1-β) 가 생략된 공식을 자주 본다. (개인적으로는 (1-β) 있는 것을 선호)     
   v<sub>dW</sub> = β v<sub>dW</sub> +dW  
   v<sub>db</sub> = β v<sub>db</sub> +db
