## 4.1 Convolutional Neural Networks

### 8. Pooling layers

- 표현 크기를 줄임으로 계산 속도를 높이고 특성 더 잘 검출
- ex) max pooling (4x4 -> 2x2)  
  ![image](https://github.com/user-attachments/assets/fd8209b2-c326-4765-aedc-6a9709845ec2)  
  hyperparameters : f = 2, s = 2

- 최대 풀링을 사용한 이유는 많은 실험에서 **성능이 좋고 직관력**이 있어서  
  -> 합성곱 신경망에서 최대 풀링이 잘 작동하는 정확한 이유는 아직 모름  
- 특이점 : 학습할 수 있는 변수가 없어 경사하강에 대해 배울 게 없음 (= 변수 고정) 

- ex2) 5x5 -> 3x3 (f = 3 , s = 1, Max pooling)  
  ![image](https://github.com/user-attachments/assets/156e91d7-2d6c-4352-a640-bdcb6df5ab75)  

- pooling **output** size = **[(n+2p-f)/s]+1** 
- max pooling 에서는 **padding 거~의 사용 안함**
- Average pooling 이라고 평균 값 취하는 pooling 이 있는데, 이건 잘 사용안함  
	(신경망 아주 아주 깊은 곳에서 어쩌다 한번씩 사용)

### 9. 합성곱 신경망 예제
- 32 x 32 x 3 손글씨 이미지 input  
  (LeNet-5 이라는 고전 신경망 따라)

- input -> f = 5, s = 1, p = 0, 필터 6개 +bias(편향) +ReLU(비선형성) -> CONV1 (28x28x6)  
  CONV1 -> f = 2, s = 2, p = 0, maxpooling -> POOL1 (14x14x6)

  - 여기에서 (CONV1 + POOL1) 이 과정을 1개의 레이어로 볼지 CONV1 / POOL1 각각의 레이어로 2개로 볼지  
    일반적으로 신경망 레이어 개수 말할 땐 가중치, 변수 가지는 레이어를 말하는데,  
    풀링 레이어는 하이퍼파라미터만 있으니 레이어를 1개로 보자. (가끔 논문, 문헌에서 2개로 보기도)  

- POOL1 -> f=5 , s=1, 필터 16개 -> CONV2 (10x10x16)  
  CONV2 -> f=2, s=2, maxpooling -> POOL2 (5x5x16)  
  POOL2 (5x5x16) = 400x1 벡터로 펼쳐보자  
  400x1 -> 120 FC3 로 만들기  
  120 -> 84 -> 0 softmax (10 output - 손글씨 0~9)  
  ![image](https://github.com/user-attachments/assets/e24c58b4-a479-484a-b573-2f5ced901ea0)  
  ![image](https://github.com/user-attachments/assets/416c7153-e6c0-47f3-8c76-7c88f00a1c3a)  

- pooling 은 parameter 없음
- Activation Size 작아지고 결국 softmax

### 10. 합성곱 신경망 장점, 훈련 방법
- 장점
1. 파라미터 공유(Parameter Sharing)  
  동일한 필터를 이미지의 여러 위치에서 사용할 수 있어 **파라미터 수 줄어**들 수 있음  
2. 연결의 희소성(Sparsity of Connections)  
   출력 값이 특정 입력 값에만 의존하고, 다른 입력값에는 영향 X  
   = 입력값의 일부만 사용해서 **불필요한 연결 감소**  
3. 변환 불변성(Translation Invariance)  
   이미지가 약간 **이동**하더라도 **동일한 특징** 인식 가능  
ex) 32x32x3 image 5x5 filter 6개  
   기존 신경망: 3,072 x 4,704 = 약 **1,400만**개의 파라미터 필요  
   convolution layer (합성곱) : ( 5 x 5 +1(bias) )x 6 = **156**개의 파라미터
   
- 훈련 방법  
1. 구조 설계  
   이미지 **입력** 후 여러 **convolution, pooling** 레이어를 거쳐 **softmax** 출력층 통해 **예측값 출력**   
   각 레이어는 다양한 파라미터(W)와 편향(B)를 가짐  
2. 손실 함수 정의  
   파라미터(W, B)를 임의로 초기화한 후, 네트워크의 예측 값과 실제 값을 비교하여 **손실 함수(J)를 계산**  
   손실 함수는 전체 학습 데이터에 대해 **손실 값의 합**을 구하고, 이를 **데이터 수(m)로 나눔**  
3. 최적화 알고리즘 사용  
   **경사 하강법**, 모멘텀을 이용한 경사 하강법, RMSProp, **Adam** 등 알고리즘을 사용, 파라미터를 **최적화**  
   손실 함수 **J를 최소화**하는 방향으로 **파라미터**(W, B)를 **업데이트**
