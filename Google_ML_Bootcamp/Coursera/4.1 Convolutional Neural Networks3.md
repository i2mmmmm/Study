## 4.1 Convolutional Neural Networks

### 8. Pooling layers

- 표현 크기를 줄임으로 계산 속도를 높이고 특성 더 잘 검출
- ex) max pooling (4x4 -> 2x2)  
  ![image](https://github.com/user-attachments/assets/fd8209b2-c326-4765-aedc-6a9709845ec2)  
  hyperparameters : f = 2, s = 2

- 최대 풀링을 사용한 이유는 많은 실험에서 **성능이 좋고 직관력**이 있어서  
  -> 합성곱 신경망에서 최대 풀링이 잘 작동하는 정확한 이유는 아직 모름  
- 특이점 : 학습할 수 있는 변수가 없어 경사하강에 대해 배울 게 없음 (= 변수 고정) 

- ex2) 5x5 -> 3x3 (f = 3 , s = 1, Max pooling)  
  ![image](https://github.com/user-attachments/assets/156e91d7-2d6c-4352-a640-bdcb6df5ab75)  

- pooling **output** size = **[(n+2p-f)/s]+1** 
- max pooling 에서는 **padding 거~의 사용 안함**
- Average pooling 이라고 평균 값 취하는 pooling 이 있는데, 이건 잘 사용안함  
	(신경망 아주 아주 깊은 곳에서 어쩌다 한번씩 사용)

### 9. 합성곱 신경망 예제
- 32 x 32 x 3 손글씨 이미지 input  
  (LeNet-5 이라는 고전 신경망 따라)

- input -> f = 5, s = 1, p = 0, 필터 6개 +bias(편향) +ReLU(비선형성) -> CONV1 (28x28x6)  
  CONV1 -> f = 2, s = 2, p = 0, maxpooling -> POOL1 (14x14x6)

  - 여기에서 (CONV1 + POOL1) 이 과정을 1개의 레이어로 볼지 CONV1 / POOL1 각각의 레이어로 2개로 볼지  
    일반적으로 신경망 레이어 개수 말할 땐 가중치, 변수 가지는 레이어를 말하는데,  
    풀링 레이어는 하이퍼파라미터만 있으니 레이어를 1개로 보자. (가끔 논문, 문헌에서 2개로 보기도)  

- POOL1 -> f=5 , s=1, 필터 16개 -> CONV2 (10x10x16)  
  CONV2 -> f=2, s=2, maxpooling -> POOL2 (5x5x16)  
  POOL2 (5x5x16) = 400x1 벡터로 펼쳐보자  
  400x1 -> 120 FC3 로 만들기  
  120 -> 84 -> 0 softmax (10 output - 손글씨 0~9)  
  ![image](https://github.com/user-attachments/assets/e24c58b4-a479-484a-b573-2f5ced901ea0)  
  ![image](https://github.com/user-attachments/assets/416c7153-e6c0-47f3-8c76-7c88f00a1c3a)  

- pooling 은 parameter 없음
- Activation Size 작아지고 결국 softmax

  
