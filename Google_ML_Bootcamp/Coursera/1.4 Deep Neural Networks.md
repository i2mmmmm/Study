## 1.4 심층 신경망

### 1. 나만의 인공신경망 만들기

몇개의 은닉층(hidden layer)을 가질지 뉴런의 수는 몇개로 할지 결정해보자    

![image](https://github.com/i2mmmmm/Study/assets/106386971/e3d060e0-aa7a-4fdd-8054-437f850b93ae)

- 4 layer NN (L = 4)
- 3 hidden layer
- x1,x2,x3 = layer 0 (input)
- hidden layer 의 뉴런 수는 각각 5,5,3
- n<sup>[ℓ]</sup> = units in layer  
- a<sup>[ℓ]</sup> = activation in layer = g<sup>[ℓ]</sup>(z<sup>[ℓ]</sup>)  
- w<sup>[ℓ]</sup> = weights for z<sup>[ℓ]</sup>

- 순방향 전파  
z<sup>[1]</sup> = w<sup>[1]</sup>X+b<sup>[1]</sup>  -> layer 1  
a<sup>[1]</sup> = g<sup>[1]</sup>(z<sup>[1]</sup>)

  z<sup>[2]</sup> = w<sup>[2]</sup>a<sup>[1]</sup>+b<sup>[2]</sup>  
  a<sup>[2]</sup> = g<sup>[2]</sup>(z<sup>[2]</sup>)  
  ... a<sup>[4]</sup> = g<sup>[4]</sup>(z<sup>[4]</sup>) = ŷ

- 순방향 전파식  
z<sup>[ℓ]</sup> = w<sup>[ℓ]</sup>a<sup>[ℓ-1]</sup>+b<sup>[ℓ]</sup>  
a<sup>[ℓ]</sup> = g<sup>[ℓ]</sup>(z<sup>[ℓ]</sup>)  

- 순방향 벡터화  
Z<sup>[ℓ]</sup> = W<sup>[ℓ]</sup>A<sup>[ℓ-1]</sup>+b<sup>[ℓ]</sup>  
A<sup>[ℓ]</sup> = g<sup>[ℓ]</sup>(Z<sup>[ℓ]</sup>)  
- 이 부분에서 for ℓ=1...4 를 for 없이 구현 불가 -> for 사용

코드의 정확성을 확인하기 위해 자주 사용하는 디버깅 도구 중 하나는  
종이에 작업중인 행렬의 차원을 적어보는 것

  z<sup>[1]</sup> = w<sup>[1]</sup>X+b<sup>[1]</sup>  
->(3,1)　(3,2)(2,1)　(3,1)  
->(n<sup>[1]</sup>,1)(n<sup>[1]</sup>,n<sup>[0]</sup>)(n<sup>[0]</sup>,1)(n<sup>[1]</sup>,1)

- dimension  
  w<sup>[ℓ]</sup> : (n<sup>[ℓ]</sup>,n<sup>[ℓ-1]</sup>)  
  b<sup>[ℓ]</sup> : (n<sup>[ℓ]</sup>,1)
  
  역전파 할 때  
    dw<sup>[ℓ]</sup> : (n<sup>[ℓ]</sup>,n<sup>[ℓ-1]</sup>)  
    db<sup>[ℓ]</sup> : (n<sup>[ℓ]</sup>,1)  -> w,b 와 같음  
  
  벡터화  
    Z<sup>[ℓ]</sup> = W<sup>[ℓ]</sup>X+b<sup>[1]</sup>  
    ->(n<sup>[ℓ]</sup>,m)(n<sup>[ℓ]</sup>,n<sup>[ℓ-1]</sup>)(n<sup>[ℓ]</sup>,m)(n<sup>[ℓ]</sup>,m)
