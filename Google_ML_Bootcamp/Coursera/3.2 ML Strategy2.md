## 3.2 ML Strategy

### 3. train-dev set

- training error 1% / dev error 10%
1. 알고리즘이 train set 에서는 데이터를 봤지만 dev set 에서는 아니었다는 것
2. dev set 데이터 분포가 다르다는 것

- 해결하기 위해 하나의 set을 더 만들어보자
  - training-dev set : same distribution as training set, but not use for training  
    -> training set 의 부분집합 = train set 과 동일 분포

- train / dev / test　　||||　　train / train_dev / dev / test

- ex)  
train error **1%**
train-dev error **9%**  
dev error **10%**  
-> train, train-dev 두 set은 같은 분포를 가진 데이터인데 이렇게 error 차이가 있다는 건  
-> 분산의 문제가 있음

train error **1%**  
train-dev error **1.5%**  
dev error **10%**  
-> 처음보는 dev set을 보고 에러 급상승
-> **데이터 불일치** 문제 (train, dev 사이의)

human error **0%**  
train error **10%**  
train-dev error **11%**  
dev error **12%**  
-> 편향 문제 (회피가능 편향 높음) = **높은 편향** 설정

human error **0%**  
train error **10%**  
train-dev error **11%**  
dev error **20%**  
-> 분산은 작지만 회피가능 **편향**은 높고, **데이터 불일치** 문제도 높음

-> dev error / test error 차이가 크면 **dev set 과적합** 정도도 알 수 있음
-> 그러면 더 **큰 dev set**을 찾아야 함 

human error **4%**  
train error **7%**  
train-dev error **10%**  
dev error **6%**  
test error **6%**  
-> 음성 인식에서 **dev set 이 train set 보다 쉬우면** 간혹 이런 경우도 있음  
ex) 다양한 대화를 test set 으로 이용했는데, dev set은 길묻기 위치묻기 같은 차량 내 음성일 경우

### 4. 데이터 불일치 문제 해결
- 수동 오류 분석을 해보자
- dev set / test set 의 차이를 알아보자
- train, dev set 을 비슷하게 하기 위해 인공 데이터 통합을 해보자
  - ex) 그냥 말하는 소리 + car noise 소리 합성  
    차에서 녹음된 것 같은 소리로 만들어 data로 활용  
    -> 이 때 문제는 10,000 시간짜리 대화 소리와 1시간 짜리 차량 노이즈 소리를 합성하면 차량 소음에 **과적합할 위험**  
    -> 수동 오류 분석 시 사람 귀로는 구별이 어려워서 조심해야 하는 문제

- **문제가 생기면**
  - training set , dev set 어떻게 다를 수 있는지
  - dev set 과 비슷한 training set data를 어떻게 더 얻을 수 있는지
  - 인공 데이터 합성 시 너무 일부에서 시뮬레이션하고 있지 않은지

### 5. transfer learning 전이학습
- 딥러닝의 큰 장점
- 이미 개발한 모델을 다른 데에도 적용시키는 것
- 모델에서 마지막 결과값 레이어 삭제 - 마지막 레이어에 준 가중치도 삭제  
  그 전 가중치한 세트에서 무작위 초기화해 새 y_hat 생성  
  ![image](https://github.com/user-attachments/assets/04d631be-9c59-4e97-8097-710bf1693ff4)  
  - 새 데이터가 별로 없는 경우 - **마지막 레이어의 가중치**만 다시 훈련 (나머지 매개변수는 고정)
  - 데이터가 충분히 있는 경우 - 나머지 신경망에 대해 **모든 레이어** 재훈련
  - 재훈련 시킬 경우 -> pre-trainnning 가중치 나중에 업데이트(미세 조정) -> fine-tunning

- 전이학습이 유용한 경우는 (A 학습 -> B) 3가지 모두 충족
1. **같은 입력값** X를 가진 경우 ( 이미지 - 이미지 / 음성 - 음성 )
2. B 보다 **A** 작업의 **데이터**가 훨씬 더 **많을** 때 ( = B 데이터가 적은데 성능을 높이고 싶을 때 )
3. A 작업의 저수준 **특성**이 B 작업 학습에 **도움이 될거**라 판단할 때


### 6. multi-task learning
- 전이학습과 달리 학습이 동시에 진행됨  
ex) 자율주행  
보행자 / 다른 차 / 정지신호 / 교통 신호 / 등등 다양한 y(i)  
y(i).shape = (4,1)  
Y.shape = (4,m)  
![image](https://github.com/user-attachments/assets/8c06d4f0-9a89-4153-8f80-737d2565b707)  
예전에 배운 건 **출력층**이 1개였는데, 지금은 **4개**!

- Loss = ∑ i=1->m ∑ j=1->4 L(y_hat, y)  
  ∑ j=1~4 이 새로 생겼다는 것  
  기존 softmax -> 단일 예시에 단일 라벨을 부여  
  현재 -> 복수 라벨을 부여  
  = 하나의 이미지로 4개의 문제를 푸는 하나의 신경망을 만드는 것

- 각각 1개씩 4개의 신경망을 만들어도 되지만  
  이미지를 인식하고 학습하는 앞 부분 학습이 비슷하다면 따로 훈련하는 것보단 효율과 성능이 더 낫다.

- 라벨이 4개 중 2개만 붙어있을 때?  
  ∑ j=1~4 에 대해 값이 0,1로 붙어있는 2개만 sum 하도록 하고 학습

- 멀티태스크 학습이 유용한 경우는
1. 공유된 **lower-lever features 가 도움을 받을 수** 있는 작업을 훈련할 때
2. 항상은 아니지만 대부분 성공한 멀티태스크 학습에서는 각 작업이 **보유한 데이터 양이 비슷**
3. 다중의 모든 작업이 잘되려면 그만큼 **큰 신경망**을 훈련시킬 때 효과적

### 7. End-to-End deep learning (종단간 학습)
- 최근 발전 중 가장 흥미로운 것
- **아주 많은 양**으로 X -> Y 학습시켜버리는 것
- ex1) 얼굴 인식
  - 기존 : 카메라가 찍는 사진에서 얼굴 위치 찾고 - 얼굴 정면으로 잘라서 - 특징을 비교하고 - 누구인지 판별
  - end-to-end : 카메라가 찍는 얼굴 - 신원조회 얼굴 비교해 판별
- ex2) 번역
  - 기존 : English -> 문장의 특성 추출 -> 많은 단계를 거쳐 -> -> -> -> French
  - end-to-end : English 문장 -> French 문장
 
  

- 
