## 1.2 신경망 (로지스틱 회귀분석)

### 3. 비용함수, 손실함수 (cost function, loss function)
**1. loss function**
- 보편적인 손실함수 = 𝓛(ŷ,y) = 1/z(ŷ-y)<sup>2</sup>  (ŷ : 알고리즘 출력, y : 실제 레이블)  
  하지만, 로지스틱에서는 이렇게 쓰지 않는다.
- **𝓛 = -(y log ŷ + (1-y) log (1-ŷ))**
- 원래 보통 제곱오차를 사용하는 이유가 실제값과 추정치의 차이가 작음을 확인하고 싶음인데  
  로지스틱에서 쓰이는 손실함수도 마찬가지로 𝓛 이 작음을 확인하는 함수.
  
ex 1) y = 1 일 때, 𝓛(ŷ,y) = -log ŷ  
  -> ŷ 은 1보다 크지 않지만, 가장 큰 값일 때 𝓛 이 가장 작아진다.  
  -> ŷ 이 1에 가까워야 함  
  -> ŷ 이 실제값 y 에 가까워진다.
  
ex 2) y = 0 일 때, 𝓛(ŷ,y) = -log (1-ŷ)  
  -> ŷ 은 0보다 작아지지 않지만, 가장 작은 값일 때 𝓛 이 가장 작아진다.  
  -> ŷ 이 0에 가까워야 함  
  -> ŷ 이 실제값 y 에 가까워진다.  
  
- 예제를 통해 비공식적 정당성을 제공 -> 추후 영상을 통해 공식적 정당성 제공할 예정

**2. Cost function**

![image](https://github.com/i2mmmmm/Study/assets/106386971/9f5ffbdb-1c8d-444c-9996-8aa47462889a)

=> 손실함수는 단일 학습 예제에 대한 오류
=> 비용함수는 전체 학습 세트에 대한 손실함수 평균

### 4. 경사하강법 (Gradient descent)

![image](https://github.com/i2mmmmm/Study/assets/106386971/8b1e0b48-f65d-48a5-b91e-11bbbaa53857)

![image](https://github.com/i2mmmmm/Study/assets/106386971/264bc51e-2289-43d8-b38b-8890bc7992e4)

- 최소값에 해당하는 W,b를 찾는 방법 = 경사하강법
  
특정 비용함수 J 는 볼록함수 = 그릇모양  
= 비용함수로 J 를 사용하는 이유 = **최소값이 하나**인 모양

![image](https://github.com/i2mmmmm/Study/assets/106386971/672bfa05-bf38-42f8-8908-67a40a6a1346)

<p>b를 무시하고 일단 W에 대해 1차원적으로 볼 때, W:= W - α $\frac{d J(W)}{dW}$</p>
<p>(α = learning rate : 한번에 얼마나 기울기 하강할 지 조절해주는 역할, $\frac{d J(W)}{dW}$ = 도함수) </p>
W:= W - α dW  -> (왼쪽, 오른쪽) 어디에서 출발해도 가운데로 수렴

<p> W:= W - α $\frac{d J(W,b)}{dW}$ ->dW (J에 대한 W의 기울기)</p>
<p> b:= b - α $\frac{d J(W,b)}{db}$ ->db (J에 대한 b의 기울기)</p>
