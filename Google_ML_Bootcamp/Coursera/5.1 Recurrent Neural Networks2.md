## 5.1 Recurrent Neural Networks (RNN)

### 4. Language model and sequence generation

- 음성인식  
  P(The apple and pair salad) = 3.2 * 10<sup>-13</sup>  
  P(The apple and pear salad) = 5.7 * 10<sup>-10</sup>  
  음성 인식된 문장이 둘 중 어떤 문장이 맞을지 확률을 계산해서 두번째 문장을 선택

- language 모델을 만들려면   
  training set : large corpus of english test  
  (corpus : 분량이 큰 or 수십개의 영어문장을 의미하는 NLP용어)

- cats average 15 hours of sleep a day 라는 문장이 있으면 이걸 먼저 토큰화 해야함  
  원핫벡터라던지  
  **<EOS>** 라는 end of sentence 라는 **문장 끝나는 지점 토큰**을 더해줌  
  전에 우리가 쓰던 10000개 단어에 **없는 단어에는 <UNK>라는 토큰**을 부여  
  그래서 저 문장은 y<sup><9></sup> = input **9개**
  
- 결국 language 모델은 각 확률을 계산하는 것  
  첫번째 단어가 나올 확률 계산하고, 첫번째 단어 나오면  
  첫번째 단어가 그거일 때 두번째 단어가 나올 확률을 계산하고 이런 식으로
  
- ex) 3개 단어로 이루어진 문장의 확률은  
  = P(y<sup><1></sup>,y<sup><2></sup>,y<sup><3></sup>) = P(y<sup><1></sup>) * P(y<sup><2></sup>|y<sup><1></sup>) * P(y<sup><3></sup>|y<sup><1></sup>,y<sup><2></sup>)

- 문자 수준 RNN (Character-Level RNN)  
  **기존** vocabulary 는 **단어 수준**의 언어모델이었는데 **문자 수준의 모델을 빌드**하면 교육 데이터의 개별 단어가 된다.  
  vocabulary = [a,b,c,...,z,0,1,2,...,9, ,.,;,A,B,...,Z]  
  Cats average ... 이 문장도 C = y1, a = y2 이런식으로 **쪼개짐**  
  그렇게 되면 **단어 토큰에 대한 걱정이 없다**


- RNN 장기의존성 문제  
  The **cat**, which already ate ...... , **was** full  
  The **cats**, which ............................ , **were** full  
  **언어**가 **장기적인 의존성**을 가지고 있음을 보여줌
  앞의 단어가 문장 **후반부에도 영향**을 미칠 수 있음  
  -> 기존 **RNN**은 의존성을 포착하는 데에 **효과적이지 X** = 기본 RNN 알고리즘의 **약점**

- 기울기 소실(Vanishing Gradient)이나 기울기 폭발(Exploding Gradient) 문제  
1. 소실 : 네트워크 깊어질수록 기울기가 초기 층에 전달되기 어려워짐 -> 앞에서 말한 장기 의존 불가  
   해결책 -> GRU(게이트 순환 유닛)나 LSTM(장기 단기 메모리) 사용해야 함  
2. 폭주 : 파라미터가 비정상적으로 커지는 현상  
   해결책 -> gradient clipping 통해 기울기가 임계값을 초과하지 않도록 조정
