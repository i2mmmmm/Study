## 5.1 Recurrent Neural Networks (RNN)

### 5. GRU ( Gated Recurrent Unit )

- RNN의 장기 의존성을 효과적으로 처리하고, 기울기 소실 문제를 해결하는데 도움
- RNN에 기억 셀, 게이트 매커니즘 추가
- memory cell (C) = 문장의 중요한 정보 기억 ex) Cat 은 단수
- update gate(Gamma_u = Γ<sub>u</sub>  
  0 과 1 사이, 셀을 업데이트할지 유지할지 조정
- 과정 : C tilde = c<sub><t></sub> : 후보 기억 셀 값 -> gate(Γ<sub>u</sub>) 값이 **1이면 업데이트, 0이면 유지**  
  (c<t> = 1 로 기억해뒀다가 필요를 끝내면 삭제)
- 수식 -> GRU 단순한 구조 -> 완전한 구조 (**Γ<sub>r</sub> 추가**)  
  ![image](https://github.com/user-attachments/assets/9d2f7cea-c100-4936-8851-b107dd5fa789)  
  Γ<sub>u</sub> = 이전 기억 셀 값이 현재 후보 셀 값에 얼마나 중요한지 결정

