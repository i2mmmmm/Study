## 5.1 Recurrent Neural Networks (RNN)

### 5. GRU ( Gated Recurrent Unit )

- RNN의 장기 의존성을 효과적으로 처리하고, 기울기 소실 문제를 해결하는데 도움
- RNN에 기억 셀, 게이트 매커니즘 추가
- memory cell (C) = 문장의 중요한 정보 기억 ex) Cat 은 단수
- update gate(Gamma_u = Γ<sub>u</sub>  
  0 과 1 사이, 셀을 업데이트할지 유지할지 조정
- 과정 : C tilde = c<sub><t></sub> : 후보 기억 셀 값 -> gate(Γ<sub>u</sub>) 값이 **1이면 업데이트, 0이면 유지**  
  (c<t> = 1 로 기억해뒀다가 필요를 끝내면 삭제)
- 수식 -> GRU 단순한 구조 -> 완전한 구조 (**Γ<sub>r</sub> 추가**)  
  ![image](https://github.com/user-attachments/assets/9d2f7cea-c100-4936-8851-b107dd5fa789)  
  Γ<sub>u</sub> = 이전 기억 셀 값이 현재 후보 셀 값에 얼마나 중요한지 결정

### 6. LSTM(long short term memory)
![image](https://github.com/user-attachments/assets/fc821033-5b03-4746-b8d0-0ad1e0eb814f)
- GRU보다 복잡하고 강력한 구조  
  **3개** 게이트 (update gate, forget gate, output gate)
- 작동 원리
  - 메모리 셀(c) 유지, 중요한 정보 남기고 불필요한 정보 잊는 방식
  - **update** gate : 새로운 정보 메모리 셀에 추가할지 결정
  - **forget** gate : 이전 메모리 셀의 값 유지할지 결정
  - **output** gate : 현재 메모리 셀 값 기반 출력값 결정

- 핍홀 연결 (peephole connection) 훔쳐보기연결?!  
  LSTM 변형 중 하나, 게이트 값 계산에 **이전 메모리 셀 값(c<sup><t-1></sup> 참조**하는 방식 (더 많은 정보 활용 가능)

- LSTM vs GRU
  - LSTM이 먼저 나오고, GRU가 늦게 나옴
  - **GRU** 장점 : 단순한 모델이라는 점 - 2개 게이트(update,reset)로 **큰 네트워크** 빌드 (**계산 빠름**)
  - **LSTM** 장점 : 게이트 3개라서 더 **유연, 성능 좋음**
  - 하나만 골라야한다면 아무래도 **안정적인 LSTM**
 
### 7. Bidirectional RNN (양방항 RNN) (BRNN)

- RNN은 이전의 정보를 가지고 다음 정보를 예측했지만, 양방향 RNN은 **과거 미래 정보 모두 사용**
- 양방향 두개의 순환 네트워크 사용  
  Forward RNN, **Backward** RNN  
- ex) He said Teddy... 일 때 기존 RNN으로는 Teddy가 사람 이름인지 테디베어인지 알 수 없지만,  
  BRNN은 뒤에 나오는 단어를 보고 알 수 있음(**정확도 ↑**)

- **NLP**(**자연어처리**)에서 자주 사용 -> **문장 단위** 입력에 효과적
- 변형해서 GRU, LSTM 에서도 양방향 RNN 구성 가능 (실제 많은 문제 **LSTM 기반 양방향 RNN**)
- 단점 : 전체 시퀀스 알아야 해서, **실시간 처리는 부적합**

![image](https://github.com/user-attachments/assets/6a9ca03b-528a-414b-9607-bd7dfcbffe3a)  
acyclic graph = 순환 그래프 ( 보라색 글씨 : forward , 초록색 글씨 : backward )


### 8. Deep RNN

- 일반적인 RNN은 1개 hidden layer 에서 처리하는데 deep RNN은 층 여러개 쌓아 복잡한 모델 구성  
  a<sup>[1]&lt;t&gt;</sup>  
  a<sup>[2]&lt;t&gt;</sup>  
  a<sup>[3]&lt;t&gt;</sup>  
  a<sup>[l]&lt;t&gt;</sup>  
이렇게 [l] 이라는 레이어를 넣어 쌓아보자

- 층마다 독립적인 파라미터 존재, 그 층에서는 동일한 시점 파라미터 적용
- RNN 은 3개 층만 쌓아도 충분히 복잡
- GRU, LSTM, BRNN 다들 깊이 쌓을 수 있음
