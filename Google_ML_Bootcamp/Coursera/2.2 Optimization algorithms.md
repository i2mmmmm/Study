## 2.2 Optimization algorithms

딥러닝에서는... 
여러 모델을 일단 트레이닝 시키고 가장 잘 작동하는 것을 찾기  
BIG 데이터여야 성능이 좋다.  
빠른 양질의 최적화 알고리즘을 갖는 것이 효율성 속도에 좋음

### 1. mini-batch Gradient Descent
- 먼저 배치 경사 하강법(Batch Gradient Descent)을 했는데  
  훈련 샘플 수가 많으면 **느리고**, 전체를 한단계씩 하려니 느리니까
- 훈련 셋을 작은 미니 배치로 **나누어서** 수행 -> **mini-batch**
- 미니 배치로 나누어 각각 순전파, 역전파, 가중치 업데이트

  ex) 500만 개의 샘플 -> 1,000개씩 나누어 5,000개 미니 배치 생성  
  X = [x<sub>1</sub>,x<sub>2</sub>,...,x<sub>m</sub>] / Y = [y<sub>1</sub>,y<sub>2</sub>...,y<sub>m</sub>]  
  -> X = [x1,...,x1000 | x1001,...,x2000 |   ...xm]  
　　　　　X<sup>{1}</sup>  　 |   　　X<sup>{2}</sup>　 　| ... 　X<sup>{5000}</sup>
```
<표기>  
x<sup>{i}</sup> : i번째 훈련 sample  
x<sup>[l]</sup> : 신경망 l번째 레이어  
X<sup>{t}</sup> : t번째 미니 배치
```
- 미니 배치 경사 하강법 알고리즘
  
1. Forward Propagation (순전파)
     Z[1] = W[1]X{t} +b[1]  
     A[1] = g[1] (z[1])   
     ...　　　　　　　　　(이 부분은 벡터화)  
     A[ℓ] = g[ℓ] (z[ℓ])

  2. Cost Function J 계산  
     미니 배치의 평균 손실 계산
     정규화 항 포함 가능

  3. Back Propagation (역전파)  
     J<sup>{t}</sup>에 대한 기울기 산출

  4. Weights Update (가중치 업데이트)

- epoch : 전체 훈련 셋 한번 통과하는 과정  
  -> **배치** 경사 하강에서는 **1** epoch 당 **1번** 업데이트  
  -> **미니배치** 경사 하강에서는 **1** epoch 당 **t번** 업데이트

- 결론 : **대량의** 데이터에서 **빠른** 훈련이 가능한 미니배치경사하강법

- 경사하강법
![image](https://github.com/user-attachments/assets/120eaec2-39bb-4f69-bd50-fd398bb93abb)



  
