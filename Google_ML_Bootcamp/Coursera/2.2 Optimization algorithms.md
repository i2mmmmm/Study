## 2.2 Optimization algorithms

딥러닝에서는... 
여러 모델을 일단 트레이닝 시키고 가장 잘 작동하는 것을 찾기  
BIG 데이터여야 성능이 좋다.  
빠른 양질의 최적화 알고리즘을 갖는 것이 효율성 속도에 좋음

### 1. mini-batch Gradient Descent
- 먼저 배치 경사 하강법(Batch Gradient Descent)을 했는데  
  훈련 샘플 수가 많으면 **느리고**, 전체를 한단계씩 하려니 느리니까
- 훈련 셋을 작은 미니 배치로 **나누어서** 수행 -> **mini-batch**
- 미니 배치로 나누어 각각 순전파, 역전파, 가중치 업데이트

  ex) 500만 개의 샘플 -> 1,000개씩 나누어 5,000개 미니 배치 생성  
  X = [x<sub>1</sub>,x<sub>2</sub>,...,x<sub>m</sub>] / Y = [y<sub>1</sub>,y<sub>2</sub>...,y<sub>m</sub>]  
  -> X = [x1,...,x1000 | x1001,...,x2000 |   ...xm]  
　　　　　X<sup>{1}</sup>  　 |   　　X<sup>{2}</sup>　 　| ... 　X<sup>{5000}</sup>
---

<표기>  
x<sup>(i)</sup> : i번째 훈련 sample  
x<sup>[l]</sup> : 신경망 l번째 레이어  
X<sup>{t}</sup> : t번째 미니 배치

----
- 미니 배치 경사 하강법 알고리즘
  
1. Forward Propagation (순전파)
     Z[1] = W[1]X{t} +b[1]  
     A[1] = g[1] (z[1])   
     ...　　　　　　　　　(이 부분은 벡터화)  
     A[ℓ] = g[ℓ] (z[ℓ])

  2. Cost Function J 계산  
     미니 배치의 평균 손실 계산
     정규화 항 포함 가능

  3. Back Propagation (역전파)  
     J<sup>{t}</sup>에 대한 기울기 산출

  4. Weights Update (가중치 업데이트)

- epoch : 전체 훈련 셋 한번 통과하는 과정  
  -> **배치** 경사 하강에서는 **1** epoch 당 **1번** 업데이트  
  -> **미니배치** 경사 하강에서는 **1** epoch 당 **t번** 업데이트

- 결론 : **대량의** 데이터에서 **빠른** 훈련이 가능한 미니배치경사하강법

- cost function 경사하강 그래프
  
![image](https://github.com/user-attachments/assets/120eaec2-39bb-4f69-bd50-fd398bb93abb)
- 미니배치의 경사하강은 매끄럽지 않을 수 있다.  
  각 미니배치에 따라 noise 가 있을 수 있고 잘못된 레이블이 있을 수도 있고, 하지만 점진적으로 감소하는 경향을 보일 것

- 미니배치 경사하강법에서 내가 결정하는 파라미터 중 하나가 **미니배치 크기**
  - if mini-batch size = m -> Batch gradient descent  -> 한번 반복에 큰 시간 소요
  - if mini-batch size = 1 -> Stochastic gradient descent -> 노이즈 크고 벡터화의 장점 무력화
  - if mini-batch size in 1 and m **[보편적인 경우]**
  - m = 2000보다 작을 때는 그냥 배치 경사 하강법 사용하길
  - 큰 훈련 셋의 경우 mini batch size t = 64 ~ 512 정도로 결정 (컴퓨터에겐 2<sup>n</sup> 형태가 효율적이라서)
  - CPU / GPU 메모리에 맞게 설정
- 결국은 다양한 시도 끝에 가장 효율적인 값을 찾는 걸 추천
  


  
