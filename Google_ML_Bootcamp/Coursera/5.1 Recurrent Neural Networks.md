## 5.1 Recurrent Neural Networks (RNN)

### 1. Sequence model (시퀀스 모델)
- 순차 데이터를 다루는 모델  
  ex) 음성인식 : X-오디오 클립, Y-텍스트 문장  
  감정분석 : X-문장(시퀀스) Y-감정 분류  
  DNA 서열 분석  
  기계 번역  
  비디오 활동 인식 등등
- X(입력), Y(출력)이 모두 시퀀스일수도 하나만 시퀀스일수도  
  X,Y 같은 길이 일수도, 다른 길이 일수도

- 표기법  
  ![image](https://github.com/user-attachments/assets/564b814f-3e23-4286-ba97-c92749a06d13)  
  입력 : X, 출력 : Y  
  X<sup>(i)⟨t⟩</sup> : i번째 샘플의 시퀀스에서 t번째 단어.

- Vocabulary 를 10000개의 단어로 구성하고 각각 고유한 벡터를 주면, 한 문장을 one-hot vector 로 표현할 수 있다  
  ![image](https://github.com/user-attachments/assets/c0f68eee-cf19-4313-995b-26b3622d0557)  
  없는 단어는 : UNK(Unknown Token)



### 2. RNN

- 기존 신경망의 문제점 : 문장 길이 달라 -> 모든 입력 출력 길이 같지 않음  
  teddy 가 사람이름 teddy 인지, teddy bear의 teddy 인지 같은 위치에 있어도 해석이 다를 수 있음
- RNN 은 순차적으로 데이터(텍스트) 처리  
  이전 단계 정보 -> 다음 단계에 전달 학습 반영  
  ![image](https://github.com/user-attachments/assets/f454a5a8-6c7d-43e7-a816-27b8f08e39f6)


- W<sub>ax</sub> : 입력 x 에서 활성화 a 로 가중치  
  W<sub>aa</sub> : 이전 활성화 a 에서 다음 활성화 a 로 가중치  
  W<sub>ay</sub>: 활성화 a 에서 출력 y 로 가중치

- 수식  
  ![image](https://github.com/user-attachments/assets/3d5c46ed-5cf4-417e-aa7f-51356ec868fc)  
  단순화를 위해 W<sub>a</sub> = [W<sub>aa</sub>,W<sub>ax</sub>] 로 병합하여 처리


### 기본 RNN (순환 신경망) 과정

1. 순전파 (Forward Propagation)
- 입력 시퀀스 x<sub>1</sub>,x<sub>2</sub>, ... ,x<sub>Tx</sub> 주어지면, 순차적으로 활성화 값 계산
- 모든 타임스텝 같은 가중치 W<sub>a</sub>, 바이어스 b<sub>a</sub> 사용
- 각 타임스텝에서 예측값 ŷ<sub>1</sub>,ŷ<sub>2</sub>, ... ,ŷ<sub>Ty</sub> 계산됨 -> 가중치 W<sub>y</sub>, 바이어스 b<sub>y</sub> 사용

2. 손실함수 (Loss Function)
- 각 타임스텝 ŷ<sub>t</sub>에 대해 cross entropy loss 사용하여 손실 계산
- 각 타임스텝의 손실 합산하여 최종 손실 L 계산  
  ![image](https://github.com/user-attachments/assets/79740a7f-1411-4c0d-adba-3944b09c3498)


3. 역전파 (Backpropagation)
- 순전파 반대방향 계산
- 각 타임스텝 손실 계산, 가중치 W<sub>a</sub>, W<sub>y</sub>, 바이어스 b<sub>a</sub>, b<sub>y</sub> 기울기 계산
- 역전파 과정은 시퀀스를 오른쪽에서 왼쪽으로 진행, 각 타임스텝에서 **파라미터 업데이트**

4. 시간을 통한 역전파 (Backpropagation Through Time) - BPTT
- 순전파는 시간이 증가하는 방향, 역전파는 시간이 **거꾸로** 이동하는 방식

### 3. RNN architectures 예제
- 입력시퀀스 x 와 출력 시퀀스 y 의 길이 T<sub>x</sub> = T<sub>y</sub> 가 같을 때 사용되는 걸 확인했지만  
  실제로 쓰일 때는 길이가 **다른** 경우가 많기 때문에 예제를 보자
1. many to many (기존에 봤던 길이가 같은 경우)  
   이름 개체 인식 (Named Entity Recognition)  
2. many to many (길이가 다른 경우)  
   기계 번역 (영어 -> 한국어 번역하면 같은 문장이어도 단어 갯수가 다를 수도)  
3. many to one  
   영화 리뷰 -> 감정분석 -> 별점이나 등급 매기기  
4. one to many  
   음악 생성 (첫 음이나 장르를 넣으면 전체 음악 시퀀스 생성)  
5. one to one  
   그냥 기본 신경망  
   
