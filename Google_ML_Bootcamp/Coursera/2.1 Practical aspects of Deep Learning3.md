### 5. Dropout Regularization

각 층의 각 노드를 무작위로 **제거**해보자.  
이때 해당 노드로 들어오고 나가는 연결도 함께 제거  
이 과정을 통해 **신경망의 크기가 줄어든다**.

노드를 제거할지 유지할지 **무작위**로 선택하는데, 이렇게 작아진 신경망도 예상보다 잘 작동!!

#### Inverted Dropout
- laeyr ℓ = 3 에서 Dropout  
  ℓ3 = np.random.rand(a3.shape[0] , a3.shape[1]) < keep_prob = 0.5,0.8 이런 **임의의 숫자**  
  여기에서는 keep_prob = 0.8 로 설정  
  -> hidden unit 을 제거할 확률 = 0.2  
  -> d3가 1일 확률 = 0.8 / 0 일 확률 = 0.2  
  -> random 으로 뽑은 난수가 0.8보다 작으면 참이 될 확률은 0.8 , 거짓이 될 확률 0.2 임을 의미

- a3 층 = activation 층 (활성화 층)  
  a3 = np.multiply(a3,d3)　　=> a3 *= d3  
  즉, **a3 /= keep.prob** 이 부분이 **dropout 핵심!**

- 예를 들어 50개 unit 이었다면 그 중 10개 unit은 shut-off 된 상태  
  z<sup>[4]</sup> = w<sup>[4]</sup>a<sup>[3]</sup>+b<sup>[4]</sup>  
  -> 이 부분에서 a3의 요소 중 20%는 0이 된다는 것  
  -> 이 때 z4의 기대값을 감소하지 않도록 (a3 /= 0.8)을 하는 것  
  -> 그럼 a3 기대값을 감소시키지 않고 -> z4 기대값도 유지

- **a3 /= keep.prob** 이 부분을 dropout 한다고 할 수 있다.  
  이렇게 하면 테스트 시간 동안 평균이 더 복잡해지는 scaling 문제를 덜 겪게 된다.
- 가장 쉬운 dropout 방식은 inverted dropout
- keep.prob 부분을 놓치면 test time 평균이 더 복잡해지니 조심


### 6. 무작위로 누락시키는 게 말이 안되는 것 같은가?

- 특정 유닛에 대한 의존도가 높으면 그 유닛이 랜덤하게 사라졌을 때 문제가 발생할 수 있으니
  -> 가중치를 분산시키자 -> 가중치의 squared norm 줄이기 -> L2 정규화와 유사한 효과를 볼 수 있음

![image](https://github.com/user-attachments/assets/a9cc4395-32ae-4d0b-b9d8-088230b49e32)

- 3개 입력 유닛/ 7개 은닉 유닛/ 7개/ 3개/ 2개/ 1개
- 이 복잡한 그림에서 keep prob 정해야 하는데 **레이어별로 다른 keep prob 설정 가능**
- 𝑤[1].shape = 7x3, w[2].shape = 7x7, w[3].shape = 3x7 ...
  -> 가중치가 가장 큰 행렬 w[2]이니까
- keep.prob -> 0.7/0.5/0.7/1.0/1.0 이렇게 주자!
- input 입력층에서도 keep.prob 줄 수 있으나 거의 대부분은 그대로 1.0 사용
- drop-out을 적용해서 가장 효과가 큰 분야는 Computer Vision
  - 고품질 데이터 수집이 어렵고, 고해상도의 데이터라 크기가 큰 편이라 과적합이 잘일어나기 때문에
  - 다른 분야에서는 드물게 사용하지만 CV 에서는 꽤 많이 사용하고 신뢰함
- drop-out 의 단점
  - 무작위 제거이다보니 비용함수 J 가 모든 반복에서 잘 정의되지는 않는다.
  - 그래서 drop-out 끄고 J 경사 확인하고 다시 drop-out을 켜서 진행하기도...

