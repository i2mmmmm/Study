### 5. Dropout Regularization

각 층의 각 노드를 무작위로 **제거**해보자.  
이때 해당 노드로 들어오고 나가는 연결도 함께 제거  
이 과정을 통해 **신경망의 크기가 줄어든다**.

노드를 제거할지 유지할지 **무작위**로 선택하는데, 이렇게 작아진 신경망도 예상보다 잘 작동!!

#### Inverted Dropout
- laeyr ℓ = 3 에서 Dropout  
  ℓ3 = np.random.rand(a3.shape[0] , a3.shape[1]) < keep_prob = 0.5,0.8 이런 **임의의 숫자**  
  여기에서는 keep_prob = 0.8 로 설정  
  -> hidden unit 을 제거할 확률 = 0.2  
  -> d3가 1일 확률 = 0.8 / 0 일 확률 = 0.2  
  -> random 으로 뽑은 난수가 0.8보다 작으면 참이 될 확률은 0.8 , 거짓이 될 확률 0.2 임을 의미

- a3 층 = activation 층 (활성화 층)  
  a3 = np.multiply(a3,d3)　　=> a3 *= d3  
  즉, **a3 /= keep.prob** 이 부분이 **dropout 핵심!**

- 예를 들어 50개 unit 이었다면 그 중 10개 unit은 shut-off 된 상태  
  z<sup>[4]</sup> = w<sup>[4]</sup>a<sup>[3]</sup>+b<sup>[4]</sup>  
  -> 이 부분에서 a3의 요소 중 20%는 0이 된다는 것  
  -> 이 때 z4의 기대값을 감소하지 않도록 (a3 /= 0.8)을 하는 것  
  -> 그럼 a3 기대값을 감소시키지 않고 -> z4 기대값도 유지

- **a3 /= keep.prob** 이 부분을 dropout 한다고 할 수 있다.  
  이렇게 하면 테스트 시간 동안 평균이 더 복잡해지는 scaling 문제를 덜 겪게 된다.
- 가장 쉬운 dropout 방식은 inverted dropout
- keep.prob 부분을 놓치면 test time 평균이 더 복잡해지니 조심


### 6. 무작위로 누락시키는 게 말이 안되는 것 같은가?

- 특정 유닛에 대한 의존도가 높으면 그 유닛이 랜덤하게 사라졌을 때 문제가 발생할 수 있으니
  -> 가중치를 분산시키자 -> 가중치의 squared norm 줄이기 -> L2 정규화와 유사한 효과를 볼 수 있음

![image](https://github.com/user-attachments/assets/a9cc4395-32ae-4d0b-b9d8-088230b49e32)

- 3개 입력 유닛/ 7개 은닉 유닛/ 7개/ 3개/ 2개/ 1개
- 이 복잡한 그림에서 keep prob 정해야 하는데 **레이어별로 다른 keep prob 설정 가능**
- 𝑤[1].shape = 7x3, w[2].shape = 7x7, w[3].shape = 3x7 ...
  -> 가중치가 가장 큰 행렬 w[2]이니까
- keep.prob -> 0.7/0.5/0.7/1.0/1.0 이렇게 주자!
- input 입력층에서도 keep.prob 줄 수 있으나 거의 대부분은 그대로 1.0 사용
- drop-out을 적용해서 가장 효과가 큰 분야는 Computer Vision
  - 고품질 데이터 수집이 어렵고, 고해상도의 데이터라 크기가 큰 편이라 과적합이 잘일어나기 때문에
  - 다른 분야에서는 드물게 사용하지만 CV 에서는 꽤 많이 사용하고 신뢰함
- drop-out 의 단점
  - 무작위 제거이다보니 비용함수 J 가 모든 반복에서 잘 정의되지는 않는다.
  - 그래서 drop-out 끄고 J 경사 확인하고 다시 drop-out을 켜서 진행하기도...


### 7. 기타 정규화 방법

**1) 데이터 증강 (Data Augmentation)**

- 고양이 이미지 예측  
  데이터 수집이 어렵고 비용이 많이 드니까  
  사진 좌우 반전으로 set 2배 늘리기  
  임의 회전, 줌인으로 늘리기  
  -> additional fake training samples 가짜 학습샘플 늘리기  
- 고양이 뿐 아니라 숫자 4 그림을 인식할 때도, 1개의 사진을 조금씩 찌그러트려 여러장을 만들기도 한다.

**2) Early Stopping**

 ![image](https://github.com/user-attachments/assets/34347792-f91f-47a6-9c30-44408ef01a77)

- 기울기 하강법을 사용할 때, train error 또는 비용 함수(J) 를 그려보면  
**점진적으로 감소**하는 그래프가 나온다.
- dev set error 또는 classification error를 그 위에 그리면
- 초반에는 감소하다가 어느 시점 이후로 다시 증가한다.

- 이 순간, dev set error 그래프의 변곡점에서 멈추는 것 = 덜 overfitting 되도록

  - 머신러닝의 여러 단계에서  
1) 비용 함수 최적화 알고리즘: RMSprop, Adam 등 다양한 최적화 알고리즘  
2) 과적합 방지(not overfitting): J(W,b)를 최소화, 과적합되지 않도록 편차줄임  
  편차 줄이는 방법으로 orthogonalization(직교화) 사용 (나중에 또 배워요~ 🙋)

  근데 Early Stopping의 단점은 이 두 가지 작업을 하나로 묶어버린다는 것.  
  기울기 하강을 일찍 정지시켜 비용 함수 최적화 전에 멈추는 것.  
  -> 그래서 보완하고자 L2를 사용하는데  
  -> 그러면 또 하이퍼파라미터 서치 넓이 분해시키고  
  -> 너무 많은 파라미터와 lambda 값을 시도해봐야하고  
  -> 비용↑ 시간↑ 힘듦

비용과 수고를 덜기 위해 earlystopping 을 다양한 값으로 시도하기도  
  (슨생님은 개인적으로 L2로 다양하게 시도하는 것을 선호)

데이터 증강과 Early Stopping에 대해 배웠다.  
다음엔 트레이닝을 빠르게 하기 위한 최적화 세팅에 대해 배워보자
