## 5.2 Natural Language Processing & Word Embeddings

### 3. learing word embedding
- 초기에는 복잡한 알고리즘을 사용했지만 요새는 **간단한** 알고리즘으로 좋은 성과 냄
- i want a glass of orange _______ 에 올 단어를 예측
- 네트워크 구조는 **embedding vector** 들을 신경망에 입력하고, **softmax** 를 통해 다음 단어를 예측
- 4개 정도의 단어를 고정해서 **문맥**으로 다음 단어를 **예측**하는 방식 -> 문장이 길어도 쉽게 처리
- 양쪽 단어, 이전 단어 등 다양한 문맥으로 단어를 유추하는데
- 요새는 **skip-gram 모델**을 사용해 단어 한 개만 사용해서 새 단어를 유추하는 **간단**하고 성과 좋은 알고리즘 사용


### 4. Word2Vec (Skip-grams model)

- Word2Vec  
  word embedding 학습하는 간단, 효율적인 방법
- Skip-grams 모델  
  문장에서 하나의 단어(문맥단어)를 선택해 그 주변 단어들 중 하나를 target 으로 설정하고 **둘 간의 관계를 학습**하는 아이디어
- context 설정해서 target 과의 관계 볼껀데,  
  - context  -> 문맥이라는 게 항상 마지막 4개 단어나 마지막 단어들이 되는 것이 아니라 **랜덤**하게 **문맥적인 단어를 선택**  
    -> ex) orange
  - target -> juice / class / my -> 이 중 어느 것도 target이 될 수 있음

- ex) vocab size = 10000  
  context c = 'orange' -> target t = 'juice'  
　　　　　　6257　　　　　　　4834
- 계산 과정  
  O<sub>c</sub> (원핫벡터)  
  -> E (특징벡터 곱해주기)  
  -> e<sub>c</sub> = E<sub>o<sub>c</sub></sub>  
  -> softmax O  
  -> y_hat (결과)

- softmax = p(t|c)  
  Θ<sub>t</sub> = parameter associated with output t  
  ![image](https://github.com/user-attachments/assets/5d673ccc-8917-447d-8036-7e6e12e29039)  
  손실함수 L(y_hat,y)

- softmax **문제** -> **계산속도**  
  p(t|c) 의 분모를 계산하는 게 상당히 느림 (10000개는 예제지만, 실제로는 훨~씬 많은 단어가 필요하므로 더 더 느림)  
  해결책 -> hierarchical softmax classifier 사용 (계층적 소프트맥스 분류)  
  마치 결정트리처럼 노드를 계속 나누는 것처럼 **단어들을 트리 구조로 분류하여 계산 복잡성을 줄임**

- how to sample the context c?  
  c 샘플링 기준에서 **the, of, a, and, to** 이런 자주 나타나는 것들은 너무 자주 나올테니 **배제하는 방향**으로 조정

### 5. Negative Sampling (Skip-grams model)

- Skip-gram model 문제점 - softmax 느린 속도의 계산 -> 개선하기 위해 negative sampling 사용
- Negative Sampling  
  - context(c) word(t) 를 **새로운 x**로 target(y)을 **새로운 y**로  
    ![image](https://github.com/user-attachments/assets/9a853611-5e7a-4619-b53e-2e2c7342a028)
  - P(y=1 | c,t) = σ(θ<sub>t</sub><sup>T</sup> e<sub>c</sub>)
- k 개의 예제가 있다면 긍정적인 예제의 비율이 k : 1  
  10000 개를 전부 사용하는 게 아니라 k 개를 사용해서 **계산 비용이 저렴**하고,  
  매 반복마다 k 개의 부정적인 예제와 1개 긍정적인 예를 사용해 **k+1개만 훈련**시킬 것.

- 알고리즘 계산 비용이 낮은 이유  
  **10000개 softmax 분류 대신** 비용 적게 드는 **k+1 이진분류(logistic)** 문제로 만드는 것  
  -> **negative sampling**
