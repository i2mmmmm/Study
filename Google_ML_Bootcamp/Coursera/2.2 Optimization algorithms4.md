## 2.2 Optimization algorithms

### 6. Learning rate decay
- 미니배치 경사하강법에서 고정된 학습률(learning rate)를 사용하면  
  최소값으로 수렴하지 못하고 주변을 배회하게 될 떄가 있음
- 그 때 학습률을 점차 줄여서 속도는 유지하면서 작은 스텝으로 만들어 정확한 수렴을 유도
- 공식  
  **α = α<sub>0</sub>(초기학습률) / (1 + decay_rate * epoch_num)**   (decay_rate : 하이퍼파라미터)
  
- ex) α<sub>0</sub> = 0.2, decay_rate = 1 이라고 하면  
  첫번째 epoch : α = 0.1,  
  두번째 epoch : α = 0.067  
  세번째 epoch : α = 0.05  
  에포크 진행에 따라 학습률 점진적 감소  
- 이외에도 지수적 감소, α = α<sub>0</sub> * 0.95<sup>epoch_num  
  ​계단형 감소 α = k/root(t) * α<sub>0</sub>  
  수동 학습률 감소 등 다양한 방법이 있다.

### 7. Local Optima

![image](https://github.com/user-attachments/assets/a2ddc2a1-d96f-440b-bb93-408cfc013356)

- 그림에서처럼 올록볼록한 형태의 그래프가 생겼을 때  
  작은 굴곡들을 **local optima** / 큰 굴곡들을 **global optima** 라고 하는데  
  알고리즘이 저 작은 굴곡 안에 갇히는 걸 걱정함 (갇히면 잘 못나옴)
- 하지만 신경망에서 대부분 기울기 0 지점은 local optima가 아닌 **saddle point** (안장점)  
  saddle point 는 한 방향은 위로 한 방향은 아래로 구부러진 안장모양의 형태  
  -> 안장점에 도달하면 **학습속도가 떨어짐**  

- 이는 최적화 문제에서 중요한 역할이며,  
  모멘텀, RMSProp, Adam과 같은 알고리즘이  
  안장점과 평탄한 지역을 **빠르게 벗어나는** 데 도움이 된다.  
  +학습률 감소, 배치 크기 조절
