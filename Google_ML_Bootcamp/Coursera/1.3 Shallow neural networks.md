## 1.3 Shallow neural networks

### 1. 표기법

<sup> [1] </sup> 레이어 숫자  
<sup> (i) </sup> i번째 훈련 예제
- 신경망 구조 -> 입력층 - 은닉층 - 출력층
- 입력층 (input layer) a<sup>[0]</sup> = X  
  (**입력층**은 몇개의 레이어로 이루어진 신경망 이라고 할 때에 **세지 않는다.**)
- 은닉층 (hidden layer) a<sup>[1]</sup><sub>1</sub>
 ~ a<sup>[1]</sup><sub>4</sub> **4개의 노드**
- 출력층 (output layer)

![image](https://github.com/i2mmmmm/Study/assets/106386971/8de8759b-8204-4eb1-9b07-f4db0cf1fd96)

- 이 행동을 할 때 for 문 쓰면 너무나도 비효율적 -> 역시 **벡터화**

### 2. 활성화 함수 
- 신경망 생성에서 선택할 수 있는 건 **신경망의 출력단위**와 **은닉층에서 사용할 활성화 함수**
- 지금까지는 시그모이드를 활성화 함수로 사용
- 하지만 주로 더 자주 사용되는 건 탄젠트 함수 또는 쌍곡선 탄젠트 함수

- 시그모이드(0~1) - 중간값 0.5 / 탄젠트 (-1~1) - 중간값 0
- 주로 탄젠트가 쓰이지만 이진 분류에서는 시그모이드가 우세(상위 레이어에 사용)  
- 두 활성화 함수의 단점으로는 z가 매우 크거나 매우 작으면 함수 기울기의 도함수가 작아짐 -> 함수의 기울기가 0에 가까워져 기울기 하강 속도가 느려짐
- 그래서 더 **보편적**으로 쓰이는 것이 max(x,z) = RELU를 default로 사용
- 요새는 leaky RELU 도 많이 사용
 
---   

### [번외] 각 활성화 함수의 장단점

1) 시그모이드 (Sigmoid)
- 장점: 출력 (0, 1) 사이에 존재하는 **이진 분류** 모델링 때 유용
- 단점: Vanishing Gradient 문제 발생 -> 역전파 과정에서 기울기가 소멸하여 학습이 느려지거나 멈출 수 있는 문제. 그리고 출력이 항상 양수이기 때문에, 다음 층으로 전달되는 값들이 편향을 가질 수 있다 = 이진 분류 외엔 굳이 사용하지 말기
2) 탄젠트 (Tanh)
- 장점: 출력 (-1, 1) 사이에 존재하여, 시그모이드 함수에 비해 출력이 중심을 기준으로 대칭적, **시그모이드 함수**에 비해 미분 값이 더 커서 Vanishing Gradient 문제가 그나마 **덜 발생**
- 단점: Vanishing Gradient 문제 그래도 발생 가능
3) ReLU (Rectified Linear Unit) a=(max(0,z))
- 장점: 단순한 최대 함수이므로 계산이 매우 **빠르고 간단**  
  Gradient 소실 문제 해결 - 음수가 아닌 입력에 대해서는 미분 값이 1이므로, 기울기 소실 문제가 줄어든다  
  입력이 음수일 때 출력이 0이 되므로, 일부 뉴런이 비활성화되어 희소한 표현을 얻을 수 있다
- 단점: Dying ReLU 문제 -> 많은 뉴런들이 음수 입력을 받아 **영원히 활성화되지 않는 상태**에 빠질 수 있음  
  출력 값이 양수만 있어 출력 값의 분포 편향 가능성
4) Leaky ReLU (a = (max(0.01z,z)))
- 장점: Dying ReLU 문제 해결 - 음수 입력에 대해 작은 기울기를 주어 뉴런이 완전히 죽지 않도록 함, 그리고 ReLU와 비슷하게 **간단하고 계산이 빠름**
- 단점: 하이퍼파라미터 선택 시 음수 기울기 값(일반적으로 0.01)을 결정해야 하는데 이 값에 따라 성능이 달라질 수 있다

---

### 3. 비선형 활성화 함수가 필요한 이유
- **선형 활성화 함수**를 사용하면 각 층의 **출력이 입력의 선형 결합**이 되면서 표현의 **한계**가 생긴다 -> 은닉층이 필요없어진다 -> 선형 관계만 학습한다
    - 선형 활성화 함수를 사용하는 게 의미가 있는 때는 g(x) = z  이런 **항등 함수** 등의 형태 또는 주택 가격 예측 등의 **선형 회귀** 문제 등 **극히 드문 특정 상황**에서만 의미가 있음
- **비선형 활성화 함수**는 신경망이 단순한 선형 모델을 넘어 **복잡한 데이터 패턴**을 학습하고, 비선형 문제를 해결하며, 계층적 특징을 효과적으로 학습할 수 있도록 한다
  - 비선형 활성화 함수가 **신경망**의 강력한 표현력과 **높은 성능**의 근간이 되므로 아주 **중요**함
