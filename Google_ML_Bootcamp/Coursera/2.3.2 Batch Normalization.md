## 2.3.2 Batch Normalization

1. batch normalization
   - 배치 정규화의 중요성  
     하이퍼파라미터 검색을 쉽게, 신경망은 더 견고하게  
     마치 입력층에서 정규화를 통해 빠른 훈련이 가능하게 하듯이, 은닉층에 하는 것  
     - 정규화  
     ![image](https://github.com/user-attachments/assets/ccb53c27-34d5-41a9-806d-06a115a816e0)

   - 배치 정규화의 효과
     - 입력 피처 x를 정규화하여 학습을 돕는 것처럼, 숨겨진 층의 z 값을 정규화하여 각 층의 학습을 돕는다.
     - 활성화 함수 (예: sigmoid)를 사용하는 경우,  
       값들이 항상 0 주변에 몰려 있지 않도록!  
       활성화 함수의 비선형성을 최대한 활용할 수 있게
     - γ와 β 파라미터를 통해 숨겨진 유닛 값들의 평균과 분산을 조절

2. 딥러닝에 활용하기
   - **X** --w[1],b[1]--> **Z[1]** ---β[1],γ[1] batch norm---> **Z̃[1]** ----> **a[1]** = **g[1](Z̃[1])** --w[2],b[2]--> **Z[2]** -->....
     - 여기서 사용된 β는 모멘텀이나 Adam 알고리즘에서 사용되는 β와는 다름
   - batch norm parameter
     - 경사 하강법 등의 최적화 알고리즘을 사용하여 β, γ 파라미터를 업데이트
     - Adam, RMSprop, 모멘텀 등의 알고리즘으로 β, γ 업데이트 가능
3. mini batch norm
   - 첫 번째 미니배치에서 Z1을 계산
   - 미니배치의 평균과 분산을 사용하여 Z1을 정규화
   - 정규화된 값을 활성화 함수에 적용하여 A1을 계산
   - 두번째, 세번째 미니배치 -> 반복하여 Z2, Z3 계산하고 정규화
   - 각 미니배치에서 배치 정규화는 해당 미니배치의 데이터만을 사용하여 Z 값을 정규화
  
   - Z[l] = W[l] * a[l-1] + b[l]로 Z를 계산하지만, 배치 정규화 과정에서 b[l]은 의미가 없어지므로 제거
   - Z[l] 정규화하여 평균 0, 분산 1로 만들고, β, γ 통해 재조정
   - Z[l] 정규화하면 b[l]의미가 없어지므로, b[l] 0으로 설정하거나 제거

