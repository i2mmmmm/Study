## 2.3 Hyperparameter tuning

### Hyperparameter tuning Guideline
1.학습률(learning rate) α : 가장 중요한 하이퍼파라미터, 다른 것보다 우선적으로 튜닝
2. 모멘텀 값 β : 0.9가 좋은 기본값, 필요에 따라 튜닝
3. Adam 에서 하이퍼파라미터 β_1, β_2, ε : 0.9, 0.999, 10<sup>-8</sup>을 사용, 튜닝 잘 안함
4. layers : 학습률 감소와 함께 중요한 요소 중 하나
5. hidden units : 모델 성능에 영향을 미칠 수 있음
6. learning rate decay : 레이어 층의 수와 함께 중요한 요소 중 하나
7. mini-batch size : 최적화 알고리즘 효율성을 위해 튜닝

- 샘플링 방법
  - 격자(grid search) : 기존 방식
    하이퍼파라미터 2가지라면 격자무늬로 같은 간격 5x5 해서 25개 포인트 샘플링
  - 랜덤(random search) : 최근 딥러닝에서 추천
