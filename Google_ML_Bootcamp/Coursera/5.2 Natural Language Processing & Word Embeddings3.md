## 5.2 Natural Language Processing & Word Embeddings

### 6. GloVe (Global Vectors for Word Representation)
- 단어 임베딩을 학습하는 알고리즘
- 기본 아이디어
  - 단어 쌍이 텍스트에서 함께 등장하는 횟수 X<sub>ij</sub>를 계산하여 단어 임베딩을 학습
  - X<sub>ij</sub> : 단어 i가 단어 j의 문맥에서 나타난 횟수
  - 문맥의 정의에 따라 X<sub>ij</sub> X<sub>ji</sub> 가 **대칭적일 수 있음**
- 목적 함수
  - <sup>(θ<sub>i</sub><sup>T</sup>e<sub>j</sub> - log(X<sub>ij</sub>))<sup>2</sup></sup>를 **최소**화하여 두 단어가 함께 나타날 확률을 잘 **예측하는 벡터**를 학습
  - θ<sub>i</sub>와 e<sub>j</sub>는 단어 <sub>i</sub>와 <sub>j</sub>의 임베딩 벡터
- 가중치 함수 F(X<sub>ij</sub>)
  - X<sub>ij</sub> = 0일 때 log(0) 문제를 해결하기 위해 사용
  - 빈도가 **낮은** 단어와 빈도가 **높은** 단어에 적절한 **가중치**를 부여
- θ<sub>i</sub>와 e<sub>j</sub>는 대칭적 역할을 하며, 학습 후 두 벡터를 평균 내어 최종 임베딩 생성
- 장점 : **간단**한 구조지만 의미 있는 단어 임베딩 학습 가능
- 단점 : 임베딩 벡터의 개별 요소는 명확한 **해석이 어려움**(특정 의미 축과 정렬되지 않음)
- 다양한 NLP 작업(예: 감정 분류)에 활용 가능
