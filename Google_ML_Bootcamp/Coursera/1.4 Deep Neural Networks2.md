### 1.4 심층 신경망

memo


![image](https://github.com/i2mmmmm/Study/assets/106386971/ae46ef8f-0133-46f3-8d4d-0b69392563ad)


역전파

dz<sup>[l]</sup> = da<sup>[l]</sup>*g'<sup>[l]</sup>(z<sup>[l]</sup>)  
dw<sup>[l]</sup> = dz<sup>[l]</sup>a<sup>[l-1]T</sup>  
db<sup>[l]</sup> = dz<sup>[l]</sup>  
da<sup>[l-1]</sup>=W<sup>[l]T</sup>dz<sup>[l]</sup>  

dz<sup>[l]</sup> = w<sup>[l+1]</sup>dz<sup>[l+1]</sup>*g<sup>[l]</sup>

벡터화

dZ<sup>[l]</sup> = dA<sup>[l]</sup>*g'<sup>[l]</sup>(Z<sup>[l]</sup>)  
dW<sup>[l]</sup> = 1/m dZ<sup>[l]</sup>A<sup>[l-1]T</sup>  
db<sup>[l]</sup> = 1/m np.sum(dZ<sup>[l]</sup>, axis =1, keepdims=True)  
dA<sup>[l-1]</sup>=W<sup>[l]T</sup>dZ<sup>[l]</sup>

X -> ReLU -> ReLU -> Sigmoid -> y_hat -> L(yhat,y) (손실계산)
역전파

da[l] = -y/a+ (1-y)/(1-a)


- hyper parameter
= W와 b 를 컨트롤 하는 것들
1) learning rate alpha (real parameters)  
iterations  
hidden layers L  
hidden units n[1],n[2]  
choice of activation function

momentum minibatch size 등등 의 파라미터는 다음에 봅시다

가장 좋은 하이퍼 파라미터를 미리 알기는 어려우므로  
다양한 값을 시도해보는 수 밖에


- 딥러닝과 뇌의 관계
단일 뉴런과 단순한 신경망 그림이 닮기는 했으나  
아직 단일 뉴런이 무얼 하는지 완벽하게 밝혀진 바가 없어,  
이해하지 못하는 부분도 많고 뇌의 학습원리를 정확히 알지 못한다  
인간의 뇌에서 아이디어를 얻어 신경망을 만들었을지도 모르지만 뇌에 대한 비유는 점차 덜 사용하는 추세

