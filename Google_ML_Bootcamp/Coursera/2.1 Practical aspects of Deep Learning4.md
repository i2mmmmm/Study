### 2.1.2 최적화

#### 1. 입력값 정규화

신경망 훈련에서 **속도를 높이는** 또 다른 방법은 **입력을 정규화**하는 것

입력값 정규화는 2단계
- 1. **평균 0**으로 만들기 (입력값에서 평균값 빼기)  
![image](https://github.com/user-attachments/assets/a17459bf-4d4c-41d4-abcb-f3c34b3b4f1a)    
x := x-μ  
- 2. **분산**을 정규화 (표준화)  
입력값을 **표준편차(σ)**로 나누어 정규화 = **분산이 1이 되도록**  
x /= σ  
**단, train set / test set 을 다르게 정규화 하면 안됨** = 같은 μ, σ 를 사용해서 정규화 


- 입력값의 정규화는 왜 하는 걸까  
![image](https://github.com/user-attachments/assets/c9048e86-f32f-4d8d-8159-048cdf451789)   
정규화하지 않으면 J(비용함수)는 아주 찌그러진 막대로 그려진다  
-> 정규화한다면 훨씬 대칭적으로 예쁜 그림이 탄생  
  -> 경사하강법이 **최소값을 향해** 진동없이 **빠르게 전진** 가능  
    -> 알고리즘 학습 **속도가 빨라짐**  
      = 비슷한 규모의 특징이라면 덜 중요한 단계가 되겠지만 대부분은 사용하는 게 좋아짐

#### 2. Vanishing / Exploding Gradients

신경망 훈련에서 가장 큰 문제점 중 하나  
특히 심층 신경망에서!  
 => 데이터 소실 또는 기울기가 폭발하는 경우

- 신경망이 깊어질수록 미분값이 매우 커지거나(그래디언트 폭발 Exploding) 작아지는(그래디언트 소실 Vanishing) 현상
  - 폭발의 경우 불안정하거나 발산하고
  - 소실의 경우 제대로 학습되지 않는다.

* 그래디언트(Gradient) = 함수의 기울기 = 손실 함수(Loss function)를 가중치(weight)에 대해 편미분한 값

ex) 가중치 행렬 = 1.5 같은 큰 값  
  -> 1.5를 L-1번 곱한 값이 y_hat 으로 출력

ex) 가중치 행렬 = 0.5 같은 작은 값  
  -> 0.5를 L-1번 곱한 값이 y_hat으로 출력

= **출력값**이 너무 커지거나 너무 작아져서 **문제**가 되는 것

이 문제가 큰 장벽으로 작용했으나 큰 도움이 되는 해결책은 **가중을 신중하게 초기화**하는 것!


#### 3. 부분적 해결법
= 무작위 초기화 선택을 조심스럽게 하는 것

![image](https://github.com/user-attachments/assets/18102ccc-04aa-4e00-a8c6-a024fdbc015c)

가중치 W를 너무 크게 초기화하면 z 가 매우 커지거나  
너무 작게 초기화하면 z 가 매우 작아질 수 있다.

-> 그래서 **W의 분산**을 조정!!  
Var(W) = 1/n (n: 신경세포로 들어가는 입력 특성의 수)

예제에서 보여준 W<sup>[l]</sup> = np.random.randn(shape) * np.sqrt(1/n<sup>[l-1]</sup>)

- ReLu 함수를 사용한다면 Var(W) = **2/n**
- tanh 함수 사용시 1/n<sup>[l-1]</sup>
- Xavier 초기화 √(2/n<sup>[l-1]</sup>+n<sup>[l]</sup>)
  - (tanh 또는 sigmoid와 같은 선형적인 활성화 함수에 적합)
