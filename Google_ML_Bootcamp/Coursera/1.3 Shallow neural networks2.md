## 1.3 Shallow neural networks

### 4. 활성화 함수 기울기 계산 방법

**1. Sigmoid**
![image](https://github.com/i2mmmmm/Study/assets/106386971/bc0d2016-0aa7-4ed9-a2e2-29fc0564ac06)

<p>함수 : g(z) = $\frac{1}{1+e^{-z}}$ = a</p>
도함수 = 기울기 = g'(z) = g(z)(1-g(z)) = a(1-a)
<br/><br/>

**2. tanh**
![image](https://github.com/i2mmmmm/Study/assets/106386971/ea363dd8-67f4-4504-af74-49087fd0b21b)

<p>함수 : g(z) = tanh(z) = $\frac{e^z-e^{-z}}{e^z+e^{-z}}$ = a</p>
도함수 = 기울기 = g'(z) = 1-(tanh(z))<sup>2</sup> = 1-a<sup>2</sup>
<br/><br/>

**3. ReLU**
![image](https://github.com/i2mmmmm/Study/assets/106386971/c5619182-6f02-442f-9335-e0d979493a48)

함수 = g(z) = max(0,z)

도함수 = 기울기 = g'(z) { 0 if z<0, 1 if z>0 }
<br/><br/>

**4. Leaky ReLU**
![image](https://github.com/i2mmmmm/Study/assets/106386971/e45a6aee-2c73-4560-9e3e-d7fe00154da9)

함수 = g(z) = max(0.01z,z)

도함수 = 기울기 = g'(z) { 0.01 if z<0, 1 if z>0 }
<br/><br/>

### 5. 그라데이션 하강?!

은닉층으로 신경망에 대한 경사하강법을 구현하는 것  
역전파를 하거나 경사 하강을 구현해야 할 방정식을 알려주겠음  

은닉층 1개 -> W[1] b[1] W[2] b[2]   
nx = n[0],n[1],n[2]  

신경망을 위한 그라데이션 하강 강의 다시보기 ㅇ_ㅇ

### 6. 랜덤 가중치

- 신경망을 바꿀 때는 가중치를 **랜덤으로 초기화**하는 것이 좋다.
- but W를 모두 0으로 초기화하는 건 NO
- W<sup>[1]</sup> = np.random.randn((2,2)) * 0.01 -> 아주 작은 값 랜덤 초기화를 위해 0.01을 상수로
  b<sup>[1]</sup> = np.zero((2,1)) -> b는 대칭성 문제가 없으므로 그냥 0으로 초기화
- 이렇게 해서 대칭성 파괴 문제 없도록 하자

- 가중치를 작은 값으로 초기화하는 것을 선호하는 이유는  
  **tanh, 시그모이드** 등의 출력 레이어에서 가중치가 너무 크면 활성화 값을 계산할 때  
  기울기가 작은 부분으로 넘어가버려서 학습속도가 느려질 수 있으므로.
  물론 시그모이드나 tanh 활성화 함수가 신경망 전체에 없다면 문제는 안되지만
  저 함수에서는 꼭 필수로 작은 가중치가 **중요**

하지만 심층 신경망에서는 다른 상수를 선택하고 싶어질텐데 그건 다음 시간에...
