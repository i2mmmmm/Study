## 1.3 Shallow neural networks

### 4. 활성화 함수 기울기 계산 방법

**1. Sigmoid**
![image](https://github.com/i2mmmmm/Study/assets/106386971/bc0d2016-0aa7-4ed9-a2e2-29fc0564ac06)

<p>함수 : g(z) = $\frac{1}{1+e^{-z}}$ = a</p>
도함수 = 기울기 = g'(z) = g(z)(1-g(z)) = a(1-a)
<br/><br/>

**2. tanh**
![image](https://github.com/i2mmmmm/Study/assets/106386971/ea363dd8-67f4-4504-af74-49087fd0b21b)

<p>함수 : g(z) = tanh(z) = $\frac{e^z-e^{-z}}{e^z+e^{-z}}$ = a</p>
도함수 = 기울기 = g'(z) = 1-(tanh(z))<sup>2</sup> = 1-a<sup>2</sup>
<br/><br/>

**3. ReLU**
![image](https://github.com/i2mmmmm/Study/assets/106386971/c5619182-6f02-442f-9335-e0d979493a48)

함수 = g(z) = max(0,z)

도함수 = 기울기 = g'(z) { 0 if z<0, 1 if z>0 }
<br/><br/>

**4. Leaky ReLU**
![image](https://github.com/i2mmmmm/Study/assets/106386971/e45a6aee-2c73-4560-9e3e-d7fe00154da9)

함수 = g(z) = max(0.01z,z)

도함수 = 기울기 = g'(z) { 0.01 if z<0, 1 if z>0 }
<br/><br/>

### 5. 그라데이션 하강?!

은닉층으로 신경망에 대한 경사하강법을 구현하는 것
역전파를 하거나 경사 하강을 구현해야 할 방정식을 알려주겠음

은닉층 1개 -> W[1] b[1] W[2] b[2] 
nx = n[0],n[1],n[2]

신경망을 위한 그라데이션 하강 강의 다시보기 ㅇ_ㅇ
