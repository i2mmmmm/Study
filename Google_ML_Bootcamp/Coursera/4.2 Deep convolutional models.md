## 4.2 Deep convolutional models

### 1. Residual Networks (ResNet) 잔류블럭

- 깊은 신경망 훈련은 어려움
	- 기울기 **소실**(vanishing gradient), **폭발**(exploding gradient) 문제  
	  -> 해결하기 위해 한 계층 활성화를 더 **깊은 층으로 건너뛰기** 연결
- 기존 네트워크 : plain network : a<sup>[l]</sup> -> linear -> ReLU - a<sup>[l+1]</sup>-> linear -> ReLU -> a<sup>[l+2]</sup>
- Residual Networks : ResNet : a<sup>[l]</sup> ----------------------------------------> ReLU -> a<sup>[l+2]</sup>

- **잔류블럭**(residual blocks)을 많이 쌓아서 심층 신경망을 형성

- 이론과 달리 실제로는 너무 깊은 신경망을 만들면 오류(training error)가 커지고 역효과  
  -> **ResNet 사용**으로 네트워크 **깊어져도 오류** 지속적 **감소** 가능
