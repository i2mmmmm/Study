## 4.2 Deep convolutional models

### 1. Residual Networks (ResNet) 잔류블럭

- 깊은 신경망 훈련은 어려움
	- 기울기 **소실**(vanishing gradient), **폭발**(exploding gradient) 문제  
	  -> 해결하기 위해 한 계층 활성화를 더 **깊은 층으로 건너뛰기** 연결
- 기존 네트워크 : plain network : a<sup>[l]</sup> -> linear -> ReLU - a<sup>[l+1]</sup>-> linear -> ReLU -> a<sup>[l+2]</sup>
- Residual Networks : ResNet : a<sup>[l]</sup> ----------------------------------------> ReLU -> a<sup>[l+2]</sup>

- **잔류블럭**(residual blocks)을 많이 쌓아서 심층 신경망을 형성

- 이론과 달리 실제로는 너무 깊은 신경망을 만들면 오류(training error)가 커지고 역효과  
  -> **ResNet 사용**으로 네트워크 **깊어져도 오류** 지속적 **감소** 가능


- a<sup>[l+2]</sup> = g(z<sup>[l+2]</sup>+a<sup>[l]</sup>)  
　　= g(w<sup>[l+2]</sup>a<sup>[l+1]</sup>+b<sup>[l+2]</sup>+a<sup>[l]</sup>   
  -> w<sup>[l+1]</sup> = 0, b<sup>[l+2]</sup> = 0 이면 항등함수를 학습해버림  
  -> a<sup>[l+2]</sup> = a<sup>[l]</sup> 이 되어 추가된 층에 부정적 영향 X  
  -> 그렇기 때문에 깊어져도 기존 성능 유지 가능
  
- ResNet은 이미지 처리에서도 매우 효과적
- Harlow 논문에서 그림 발췌  
  ![image](https://github.com/user-attachments/assets/937b0a99-c302-4988-91ad-c205b3cbe3e8)
- 기본적으로 여러 개의 3x3 same convolution 레이어가 사용되며, 이를 통해 차원이 보존  
  -> ex) Conv Conv Conv pull 구조를 반복해 이미지에서 특징 추출, 마지막에 Fully Connected Layer를 사용해 Softmax로 분류

  ### 2. 1x1 convolution
-  이미지의 각 위치에서 한 채널의 값을 필터의 값과 곱하는 연산
  -> 단일 채널(ex-> 6x6x1 image)의 경우, 단순히 값을 곱하는 게 된다
  -> 다중 채널에서는 복잡한 연산
   
