## 5.2 Natural Language Processing & Word Embeddings


### 1. Word Embedding

- 기존에 배운 방식 = **word representation**  
  V= [a, aaron, ... , zula, <UNK>]  
  원핫벡터 (1-hot representation)  
  이 방식의 단점은 I want a glass of orange juice 라는 문장을 학습한 모델이 있다해도  
  I want a glass of apple ______. 에서 빈칸에 **juice를 연상하기는 쉽지 않다**.  
  -> 원핫벡터 방식이라 orange 와 apple 사이의 **내적이 0이기 때문에**

- **Word Embedding (단어 임베딩)**  
  그래서 각 단어에 각각의 특징과 값을 학습하도록 해보려고 **특징 벡터** 만들어 봄  
  ![image](https://github.com/user-attachments/assets/ff1bc536-5568-49a8-9fc3-1945624339c2)  
  특징이 300개라고 하면  
  man 의 특징은 e<sub>5391</sub>로 나타내며 300차원 벡터로 표현  
  그럼 이제 orange와 apple 이 비슷하다는 걸 알 수 있음

- 각 단어 **300차원 임베딩** 학습  
  -> 300차원 데이터를 가지고 **2차원 공간**에 임베드 -> **시각화**(visualizing word embeddings)  
  -> 이 때 사용되는 알고리즘 = **t-SNE algorithm** (from Laurens van der maaten, Geoff Hinton)  
  -> 시각화하면 비슷한 성질끼리 그룹하기 좋게 가까이 배치됨  
  ![image](https://github.com/user-attachments/assets/cfd47941-6f3d-465b-945d-f2de98275d87)

- 단어 임베딩은 NLP의 핵심 개념  
  -> 더 작은 데이터셋으로도 효과적인 NLP 모델을 구축 가능
