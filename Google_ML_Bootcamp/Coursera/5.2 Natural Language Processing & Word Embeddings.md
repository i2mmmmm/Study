## 5.2 Natural Language Processing & Word Embeddings


### 1. Word Embedding

- 기존에 배운 방식 = **word representation**  
  V= [a, aaron, ... , zula, <UNK>]  
  원핫벡터 (1-hot representation)  
  이 방식의 단점은 I want a glass of orange juice 라는 문장을 학습한 모델이 있다해도  
  I want a glass of apple ______. 에서 빈칸에 **juice를 연상하기는 쉽지 않다**.  
  -> 원핫벡터 방식이라 orange 와 apple 사이의 **내적이 0이기 때문에**

- **Word Embedding (단어 임베딩)**  
  그래서 각 단어에 각각의 특징과 값을 학습하도록 해보려고 **특징 벡터** 만들어 봄  
  ![image](https://github.com/user-attachments/assets/ff1bc536-5568-49a8-9fc3-1945624339c2)  
  특징이 300개라고 하면  
  man 의 특징은 e<sub>5391</sub>로 나타내며 300차원 벡터로 표현  
  그럼 이제 orange와 apple 이 비슷하다는 걸 알 수 있음

- 각 단어 **300차원 임베딩** 학습  
  -> 300차원 데이터를 가지고 **2차원 공간**에 임베드 -> **시각화**(visualizing word embeddings)  
  -> 이 때 사용되는 알고리즘 = **t-SNE algorithm** (from Laurens van der maaten, Geoff Hinton)  
  -> 시각화하면 비슷한 성질끼리 그룹하기 좋게 가까이 배치됨  
  ![image](https://github.com/user-attachments/assets/cfd47941-6f3d-465b-945d-f2de98275d87)

- 단어 임베딩은 NLP의 핵심 개념  
  -> 더 **작은 데이터셋**으로도 **효과적인 NLP** 모델을 구축 가능

- 새로운 단어가 나올 때,  
  ex) durian, cultivator -> 두리안이 과일이라서 오렌지와 비슷한 류이고 경작자가 농부와 비슷한 뜻이라는 것을  
  추론하기 위해서 큰 데이터 셋을 활용 (온라인에서 다운로드받은 대규모 데이터 (ex. 수억개 단어))

- 전이 학습 과정  
  1. 큰 텍스트 corpus 로부터 워드 임베딩을 배우는 것  
    (온라인에서 준비된 단어를 다운로드 받을 수도)  
  2. Transfer embedding - 작은 training 셋 포함해서  
    (좋은 점 : 원핫벡터 아닌 특징벡터라서 기존 **10000차원에서 300차원** dense 벡터로 줄였으니)  
  3. 세부 조정 (fine-tuning)  
    작은 데이터셋이면 세부 조정 생략

- NLP에서 다양하게 임베딩 활용이 가능
- 얼굴 인식과 유사함  
  얼굴 인식은 새로운 얼굴 사진에 대해 **인코딩** 생성  
  단어 임베딩은 고정된 어휘에 대해 **임베딩** 학습  
  (encoding, embedding 용어가 상호 교환적, 상당히 비슷)  

- 임베딩을 통해 **적은 레이블 데이터**로 **효과적인 모델 구축** 가능!!
